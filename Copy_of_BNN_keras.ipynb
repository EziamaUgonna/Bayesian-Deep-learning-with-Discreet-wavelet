{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BNN_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian_analysis-/blob/master/Copy_of_BNN_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfAP82fA0orB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import tensorflow as tf \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "%matplotlib inline\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwyD3STE1N0J",
        "colab_type": "code",
        "outputId": "088971d6-c160-4336-df14-e0917f9b426e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_jzzx_u1Seh",
        "colab_type": "code",
        "outputId": "20d87c06-15a3-4674-905a-bd1c26a0095d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!ls \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " acc_1.csv\t\t     elvin.csv\n",
            " acc_2.csv\t\t    'Getting started.pdf'\n",
            " acc_3.csv\t\t     kernel.ipynb\n",
            " acc_4.csv\t\t     preds_2.npy\n",
            " acc_5.csv\t\t     preds_3.npy\n",
            " Bayesian_log_trial_.ipynb   Untitled0.ipynb\n",
            " BNN_Work.ipynb\t\t     Untitled1.ipynb\n",
            " BSM11.csv\t\t     Variational_autoencoder-1-1.ipynb\n",
            " BSM3.csv\t\t     work1.csv\n",
            "'Colab Notebooks'\t     work2.csv\n",
            " data1.csv\t\t    'work _desertation.ipynb'\n",
            " data23.csv\t\t     work_main\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCiIc7jl1WTp",
        "colab_type": "code",
        "outputId": "bd90d854-a572-4815-b5e3-f7c4919d6c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read in the dataset\n",
        "df = pd.read_csv('/content/gdrive/My Drive/work2.csv').astype(np.float32)\n",
        "change = df.query('Speed>0').sample(frac = .1).index\n",
        "df.loc[change, 'Speed'] = 0\n",
        "df.loc[change, 'Class'] = 0\n",
        "df.to_csv('work2.csv', header = True, index =False)\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048575, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ibVf_rX1Way",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aANjA53f1WlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
        "#reshape y-data to become column vector \n",
        "y_train = np.reshape(y_train, [-1,1])\n",
        "y_test = np.reshape(y_test, [-1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPsanem11uKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the dataset \n",
        "scalar_x_train = StandardScaler().fit(X_train)\n",
        "X_train = scalar_x_train.transform(X_train)\n",
        "X_test  = scalar_x_train.transform(X_test)\n",
        "X_val   = scalar_x_train.transform(X_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZw99XSD40r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
        " \n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)   #Shuffle and the dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val,y_val))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "test_dataset =tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
        "test_dataset = test_dataset.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqezAEkh1zaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18XtdbLdK_3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras import activations, initializers\n",
        "from keras.layers import Layer\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def mixture_prior_params(sigma_1, sigma_2, pi, return_sigma=False):\n",
        "    params = K.variable([sigma_1, sigma_2, pi], name='mixture_prior_params')\n",
        "    sigma = np.sqrt(pi * sigma_1 ** 2 + (1 - pi) * sigma_2 ** 2)\n",
        "    return params, sigma\n",
        "\n",
        "def log_mixture_prior_prob(w):\n",
        "    comp_1_dist = tf.distributions.Normal(0.0, prior_params[0])\n",
        "    comp_2_dist = tf.distributions.Normal(0.0, prior_params[1])\n",
        "    comp_1_weight = prior_params[2]    \n",
        "    return K.log(comp_1_weight * comp_1_dist.prob(w) + (1 - comp_1_weight) * comp_2_dist.prob(w))    \n",
        "\n",
        "# Mixture prior parameters shared across DenseVariational layer instances\n",
        "prior_params, prior_sigma = mixture_prior_params(sigma_1=1.0, sigma_2=0.1, pi=0.2)\n",
        "\n",
        "class DenseVariational(Layer):\n",
        "    def __init__(self, output_dim, kl_loss_weight, activation=None, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.kl_loss_weight = kl_loss_weight\n",
        "        self.activation = activations.get(activation)\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):  \n",
        "        self._trainable_weights.append(prior_params) \n",
        "\n",
        "        self.kernel_mu = self.add_weight(name='kernel_mu', \n",
        "                                         shape=(input_shape[1], self.output_dim),\n",
        "                                         initializer=initializers.normal(stddev=prior_sigma),\n",
        "                                         trainable=True)\n",
        "        self.bias_mu = self.add_weight(name='bias_mu', \n",
        "                                       shape=(self.output_dim,),\n",
        "                                       initializer=initializers.normal(stddev=prior_sigma),\n",
        "                                       trainable=True)\n",
        "        self.kernel_rho = self.add_weight(name='kernel_rho', \n",
        "                                          shape=(input_shape[1], self.output_dim),\n",
        "                                          initializer=initializers.constant(0.0),\n",
        "                                          trainable=True)\n",
        "        self.bias_rho = self.add_weight(name='bias_rho', \n",
        "                                        shape=(self.output_dim,),\n",
        "                                        initializer=initializers.constant(0.0),\n",
        "                                        trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        kernel_sigma = tf.math.softplus(self.kernel_rho)\n",
        "        kernel = self.kernel_mu + kernel_sigma * tf.random.normal(self.kernel_mu.shape)\n",
        "\n",
        "        bias_sigma = tf.math.softplus(self.bias_rho)\n",
        "        bias = self.bias_mu + bias_sigma * tf.random.normal(self.bias_mu.shape)\n",
        "                \n",
        "        self.add_loss(self.kl_loss(kernel, self.kernel_mu, kernel_sigma) + \n",
        "                      self.kl_loss(bias, self.bias_mu, bias_sigma))\n",
        "        \n",
        "        return self.activation(K.dot(x, kernel) + bias)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)\n",
        "    \n",
        "    def kl_loss(self, w, mu, sigma):\n",
        "        variational_dist = tf.distributions.Normal(mu, sigma)\n",
        "        return kl_loss_weight * K.sum(variational_dist.log_prob(w) - log_mixture_prior_prob(w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YZV7PB4Xqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "train_size = X_train.shape[0]\n",
        "batch_size = train_size\n",
        "num_batches = train_size / batch_size\n",
        "kl_loss_weight = 1.0 / num_batches\n",
        "\n",
        "x_in = Input(shape=(19,))\n",
        "x = DenseVariational(20, kl_loss_weight=kl_loss_weight, activation='relu')(x_in)\n",
        "x = DenseVariational(20, kl_loss_weight=kl_loss_weight, activation='relu')(x)\n",
        "x = DenseVariational(1, kl_loss_weight=kl_loss_weight)(x)\n",
        "\n",
        "model = Model(x_in, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrFRmSB-04wp",
        "colab_type": "code",
        "outputId": "788cc7b6-e9d5-412e-fd9d-5fd6180a175e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "#from tf.keras import callbacks, optimizers\n",
        "noise = 1.0\n",
        "def neg_log_likelihood(y_test, y_pred, sigma=noise):\n",
        "    dist = tf.distributions.Normal(loc=y_pred, scale=sigma)\n",
        "    return tf.keras.backend.sum(-dist.log_prob(y_true))\n",
        "\n",
        "model.compile(loss=neg_log_likelihood, optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-3a8a8cfa741d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 342\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-3a8a8cfa741d>\u001b[0m in \u001b[0;36mneg_log_likelihood\u001b[0;34m(y_test, y_pred, sigma)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw2JF1qv6gZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_epochs = 1000\n",
        "\n",
        "#filepath = \"weight.best.hdf5\"\n",
        "#checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=0, save_best_only=True)\n",
        "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', min_delta=0.005, patience=20, verbose=0, restore_best_weights=True)\n",
        "reducelr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, verbose=1, min_lr=1e-7)\n",
        "model.fit(train_dataset,\n",
        "               epochs=max_epochs,steps_per_epoch=30,\n",
        "               shuffle=True,\n",
        "               verbose=2,\n",
        "               validation_data=val_dataset,validation_steps=3, callbacks = [earlystopper,reducelr] )\n",
        "\n",
        "\n",
        "#model.save(filepath)\n",
        "\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbTNIeb12DmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "\n",
        "#X_test = np.linspace(-1.5, 1.5, 1000).reshape(-1, 1)\n",
        "y_pred_list = []\n",
        "\n",
        "for i in tqdm.tqdm(range(500)):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_list.append(y_pred)\n",
        "    \n",
        "y_preds = np.concatenate(y_pred_list, axis=1)\n",
        "\n",
        "y_mean = np.mean(y_preds, axis=1)\n",
        "y_sigma = np.std(y_preds, axis=1)\n",
        "\n",
        "plt.plot(X_test, y_mean, 'r-', label='Predictive mean');\n",
        "plt.scatter(X, y, marker='+', label='Training data')\n",
        "plt.fill_between(X_test.ravel(), \n",
        "                 y_mean + 2 * y_sigma, \n",
        "                 y_mean - 2 * y_sigma, \n",
        "                 alpha=0.5, label='Epistemic uncertainty')\n",
        "plt.title('Prediction')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvcKDmoU7HWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_in = Input(shape=(19,))\n",
        "x = DenseVariational(20, kl_loss_weight=kl_loss_weight, activation='relu')(x_in)\n",
        "x = DenseVariational(20, kl_loss_weight=kl_loss_weight, activation='relu')(x)\n",
        "x = DenseVariational(1, kl_loss_weight=kl_loss_weight)(x)\n",
        "model = Model(x_in, x)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}