{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BNN_Work.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian_analysis-/blob/master/BNN_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KYS1a1J9C9pe",
        "colab_type": "code",
        "outputId": "8432bbd4-2b05-483b-dd19-c5060a11d650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install wurlitzer\n",
        "!pip install --upgrade -q gspread\n",
        "from tensorboardcolab import *\n",
        "import shutil\n",
        "#clean out the directory\n",
        "shutil.rmtree('./Graph', ignore_errors=True)\n",
        "os.mkdir('./Graph')\n",
        "tf.reset_default_graph()\n",
        "#will start the tunneling and will print out a link:\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.6/dist-packages (1.0.2)\n",
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://988013b7.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KeocMQyXDUhK",
        "colab_type": "code",
        "outputId": "a1847bfb-cddf-4349-a77e-819acef42659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "!pip install py-flags\n",
        "#from flags import Flags\n",
        "from flags import *\n",
        "!pip install python-utils \n",
        "import python_utils \n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py-flags in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: dictionaries==0.0.1 in /usr/local/lib/python3.6/dist-packages (from py-flags) (0.0.1)\n",
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from python-utils) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WQBkQAGhDG8e",
        "colab_type": "code",
        "outputId": "ff6951ff-2474-4af0-8b2e-01067b5ba729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gc\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google/colab/auth.py:140: ResourceWarning: unclosed <ssl.SSLSocket fd=59, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 41428), raddr=('74.125.206.84', 443)>\n",
            "  if _check_adc():\n",
            "/usr/local/lib/python3.6/dist-packages/gspread/__init__.py:38: ResourceWarning: unclosed <ssl.SSLSocket fd=57, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 52292), raddr=('74.125.206.95', 443)>\n",
            "  client.login()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_YLOC26ZEpr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import math\n",
        "\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OaMY_bVVEtmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RhPmxemmE00z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_worksheet(filename, sheetname):\n",
        "    #read from Google Sheets\n",
        "    rows = gc.open(filename).worksheet(sheetname).get_all_values()\n",
        "    return rows\n",
        "\n",
        "def read_worksheets(filename, sheetname, col_names):\n",
        "    datasets = []\n",
        "    for sheet in sheetname:\n",
        "        rows = read_worksheet(filename,sheet)\n",
        "        df = pd.DataFrame.from_records(rows,columns=col_names )\n",
        "        datasets.append( df )\n",
        "    return datasets\n",
        "        \n",
        "\n",
        "def merge_datasets(datasets):\n",
        "    base = datasets[0]\n",
        "    for ds in datasets[1:]:\n",
        "        #print(len(ds))\n",
        "        base = base.append(ds,ignore_index=True)\n",
        "    return base\n",
        "\n",
        "def splitting_data(dataset, ratio_1 = 0.8, ratio_2 = 0.9, shuffle= True, seed = 0):\n",
        "    split_1 = int(ratio_1*len(datasets))\n",
        "    split_2 = int(ratio_2*len(datasets))\n",
        "    if shuffle:\n",
        "        np.random.seed(seed = seed)\n",
        "        np.random.shuffle(datasets.values)\n",
        "    train = datasets[:split_1]\n",
        "    validation= datasets[split_1:split_2]\n",
        "    test = datasets[split_2:]\n",
        "    return train, validation, test\n",
        "\n",
        "#def split_dataset(dataset,ratio=0.3,shuffle=True,seed=0):\n",
        "    #cnt = len(dataset)\n",
        "    #cut = math.ceil(ratio*cnt)\n",
        "    \n",
        "    #if shuffle:\n",
        "       # np.random.seed(seed=seed)\n",
        "       # np.random.shuffle(dataset.values)\n",
        "        \n",
        "   # valid = dataset[:cut]\n",
        "    #train = dataset[cut:]\n",
        "    #return train,valid\n",
        "\n",
        "def normalize(df,column):\n",
        "    values = df[column].astype(float).values\n",
        "    mean = np.mean(values)\n",
        "    s = np.sum(values)\n",
        "    std = np.std(values)\n",
        "    norm_values = ( values - mean ) / std\n",
        "    df[column] = norm_values\n",
        "    return mean,std\n",
        "\n",
        "\n",
        "\n",
        "def normalize_columns(df,col_names):\n",
        "    means_stds = {}\n",
        "    for col in col_names:\n",
        "        means_stds[col] = normalize(df,col)\n",
        "    return means_stds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MfZ1Kc1aE2kL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#filename = 'data1.csv'\n",
        "filename = 'dataset-baysian'\n",
        "columns = 'columns'\n",
        "sheets = ['data2','data23','data59','data6']\n",
        "\n",
        "\n",
        "local_file = Path(filename+\".csv\")\n",
        "\n",
        "unified = None\n",
        "\n",
        "if not local_file.exists():\n",
        "    col_names = read_worksheet(filename,columns)[0]\n",
        "    datasets = read_worksheets(filename,sheets,col_names)\n",
        "    #display( *[df.head() for df in datasets] )\n",
        "    unified = merge_datasets(datasets)\n",
        "    unified.to_csv(local_file)\n",
        "else:\n",
        "    unified = pd.read_csv(local_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RxnsDcFE9Uy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# \"class\" != 1 => 0\n",
        "unified['class'].replace(to_replace=r'[^1]+', value='0', inplace=True, regex=True)\n",
        "\n",
        "#norm_cols = ['time', 'messageID', 'pos/0', 'pos/1', 'pos/2', 'spd/0', 'spd/1']\n",
        "#means_stds = normalize_columns(unified,norm_cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCVrt7UTE-r7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "0f030186-469b-4723-9135-6b25513296d1"
      },
      "cell_type": "code",
      "source": [
        "np.savez(\"data_main.npz\", features=unified.values.astype(np.float32), labels=labels.values.astype(np.float32))\n",
        "\n",
        "\n",
        "unified.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8e05080d4276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_main.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pCBgEn_3FEJT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfd = tf.contrib.distributions\n",
        "\n",
        "# Tuning program settings\n",
        "FLAGS = flags.FLAGS\n",
        "FLAGS.learning_rate = 0.09 # change\n",
        "FLAGS.num_hidden_layers = 7\n",
        "FLAGS.num_neurons_per_layer = 3\n",
        "FLAGS.activation_function = \"sigmoid\"\n",
        "FLAGS.num_principal_components = 7\n",
        "FLAGS.batch_size = 1000     # kept constant under hyperopt\n",
        "FLAGS.num_epochs = 10000  # kept constant under hyperopt\n",
        "\n",
        "DATASET_FILE = \"data_main.npz\" #\"drug_data.npz\"\n",
        "USE_PCA = False\n",
        "\n",
        "TRAIN_PERCENTAGE_01 = 0.8\n",
        "TRAIN_PERCENTAGE_02 = 0.9\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1iqbbc4FJhp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_input_pipeline(data_main, batch_size, number_of_principal_components):\n",
        "    \"\"\"Build an Iterator switching between train and heldout data.\n",
        "    Args:\n",
        "    `data_main.npz`: string representing the path to the .npz dataset.\n",
        "    `batch_size`: integer specifying the batch_size for the dataset.\n",
        "    `number_of_principal_components`: integer specifying how many principal components\n",
        "    to reduce the dataset into.\n",
        "    \"\"\"\n",
        "    # Build an iterator over training batches.\n",
        "    with np.load(data_main.npz) as DATASET_FILE:\n",
        "        features = DATASET_FILE[\"features\"]\n",
        "        labels = DATASET_FILE[\"labels\"]\n",
        "        \n",
        "        features = features.astype(np.float32)\n",
        "        labels = labels.astype(np.float32)\n",
        "        \n",
        "        # PCA (sklearn)\n",
        "        if USE_PCA:\n",
        "            features = PCA(n_components=number_of_principal_components).fit_transform(features)\n",
        "        \n",
        "        # Splitting into training and validation sets\n",
        "        #train_range = int(TRAIN_PERCENTAGE * len(features))\n",
        "        \n",
        "        split_1 = int(TRAIN_PERCENTAGE_01*len(features))\n",
        "        split_2 = int(TRAIN_PERCENTAGE_02 *len(features))\n",
        "        \n",
        "        training_features = features[:split_1]\n",
        "        training_labels = labels[: split_1]\n",
        "        validation_features = features[ split_1: split_2]\n",
        "        validation_labels = labels[split_1:split_2]\n",
        "        testing_features = features[split_2:]\n",
        "        test_labels      = labels[split_2:]\n",
        "        \n",
        "        # Z-normalising: (note with respect to training data)\n",
        "        training_features = (training_features - np.mean(training_features, axis=0))/np.std(training_features, axis=0)\n",
        "        validation_features = (validation_features - np.mean(training_features, axis=0))/np.std(training_features, axis=0)\n",
        "        testing_features    = (test_features -np.mean(training_features, axis=0))/np.std(training_features, axis=0)\n",
        "    # Create the tf.Dataset object\n",
        "    training_dataset = tf.data.Dataset.from_tensor_slices((training_features, training_labels))\n",
        "    \n",
        "    # Shuffle the dataset (note shuffle argument much larger than training size)\n",
        "    # and form batches of size `batch_size`\n",
        "    training_batches = training_dataset.shuffle(20000).repeat().batch(batch_size)\n",
        "    training_iterator = training_batches.make_one_shot_iterator()\n",
        "    \n",
        "    # Build a iterator over the heldout set with batch_size=heldout_size,\n",
        "    # i.e., return the entire heldout set as a constant.\n",
        "    heldout_dataset = tf.data.Dataset.from_tensor_slices((validation_features, validation_labels))\n",
        "    heldout_frozen = (heldout_dataset.take(len(validation_features)).\n",
        "                    repeat().batch(len(validation_features)))\n",
        "    heldout_iterator = heldout_frozen.make_one_shot_iterator()\n",
        "    \n",
        "    \n",
        "    #Building iterator over the testing set \n",
        "    testing_dataset = tf.data.Dataset.from_tensor_slices((testing_features, testinng_labels))\n",
        "    testing_batches = testing_dataset.shuffle(20000).repeat().batch(batch_size)\n",
        "    testing_iterator = training_batches.make_one_shot_iterator()\n",
        "    \n",
        "    # Combine these into a feedable iterator that can switch between training\n",
        "    # validation,and testing inputs.\n",
        "    # Here should the minibatch increment be defined \n",
        "    handle = tf.placeholder(tf.string, shape=[],)\n",
        "    feedable_iterator = tf.data.Iterator.from_string_handle(\n",
        "    handle, training_batches.output_types, training_batches.output_shapes)\n",
        "    features_final, labels_final = feedable_iterator.get_next()\n",
        "    \n",
        "    return features_final, labels_final, handle, training_iterator, heldout_iterator, testing_iterator, train_range\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XwMDiQDTJZcD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "59c333f1-83e4-4413-be3d-7bb1f2cf5adb"
      },
      "cell_type": "code",
      "source": [
        "def main(argv):\n",
        "  # extract the activation function from the hyperopt spec as an attribute from the tf.nn module\n",
        "    activation = getattr(tf.nn, FLAGS.activation_function)\n",
        "    # define the graph\n",
        "    with tf.Graph().as_default():\n",
        "      (features, labels, handle,\n",
        "       training_iterator, heldout_iterator, testing_iterator, train_range) = build_input_pipeline(\n",
        "            data_main.npz, FLAGS.batch_size, FLAGS.num_principal_components)\n",
        "      #Building the Bayesian Neural Network. \n",
        "      # We are here using the Gaussian Reparametrization Trick\n",
        "      # to compute the stochastic gradients as described in the paper\n",
        "    with tf.name_scope(\"bayesian_neural_net\", values=[features]):\n",
        "      neural_net = tf.keras.Sequential()\n",
        "      for i in range(FLAGS.num_hidden_layers):\n",
        "        layer = tfp.layers.DenseReparameterization(\n",
        "            units=FLAGS.num_neurons_per_layer,\n",
        "            activation=activation,\n",
        "            trainable=True,\n",
        "            kernel_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag with hyperopt sigmakernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "            kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "            kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "            bias_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag with hyperopt sigma\n",
        "            bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "            bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "            )\n",
        "         neural_net.add(layer)\n",
        "    neural_net.add(tfp.layers.DenseReparameterization(\n",
        "        units=1, # one dimensional output\n",
        "        activation=None, # since regression (outcome not bounded)\n",
        "        trainable=True, # i.e subject to optimization\n",
        "        kernel_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag\n",
        "        kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "        kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "        bias_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag\n",
        "        bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "        bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "      ))\n",
        "    predictions = neural_net(features)\n",
        "    preds = []\n",
        "    for _ in range(1000):\n",
        "      preds.append(neural_net(features))\n",
        "      MAP, var = tf.nn.moments(tf.squeeze(preds), axes=[0])\n",
        "   # Compute the -ELBO as the loss, averaged over the batch size.\n",
        "  neg_log_likelihood = tf.reduce_mean(tf.squared_difference(labels, predictions))\n",
        "  kl = sum(neural_net.losses) / FLAGS.batch_size\n",
        "  elbo_loss = kl + neg_log_likelihood\n",
        "  # Build metrics for evaluation. Predictions are formed from a single forward\n",
        "  # pass of the probabilistic layers. They are cheap but noisy predictions.\n",
        "  accuracy, accuracy_update_op = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "  labels=labels, predictions=predictions)\n",
        "  \n",
        "  with tf.name_scope(\"train\"):\n",
        "    # define optimizer - we are using (stochastic) gradient descent\n",
        "    opt = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "            \n",
        "    # define that we want to minimize the loss (-ELBO)\n",
        "    train_op = opt.minimize(elbo_loss)\n",
        "    # start the session\n",
        "    sess = tf.Session()\n",
        "    # initialize the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "\n",
        "    # Run the training loop\n",
        "    train_handle = sess.run(training_iterator.string_handle())\n",
        "    heldout_handle = sess.run(heldout_iterator.string_handle())\n",
        "    test_handle = sess.run(test_iterator.string_handle())\n",
        "    # Run the epochs\n",
        "    for epoch in range(FLAGS.num_epochs):\n",
        "      _ = sess.run([train_op, accuracy_update_op],\n",
        "                   feed_dict={handle: train_handle})\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "        loss_value, accuracy_value = sess.run(\n",
        "            [elbo_loss, accuracy], feed_dict={handle: train_handle})\n",
        "        loss_value_validation, accuracy_value_validation = sess.run(\n",
        "            [elbo_loss, accuracy], feed_dict={handle: heldout_handle} \n",
        "        )\n",
        "        print(\"Epoch: {:>3d} Loss: [{:.3f}, {:.3f}] Accuracy: [{:.3f}, {:.3f}]\".format(\n",
        "            epoch, loss_value, loss_value_validation,accuracy_value, accuracy_value_validation))\n",
        "\n",
        "    # Check if final epoch, if so return the validation loss for the last epoch             \n",
        "     if epoch == FLAGS.num_epochs-1:\n",
        "        final_loss, final_accuracy = sess.run(\n",
        "            [elbo_loss, accuracy], feed_dict={handle: heldout_handle}\n",
        "        )\n",
        "        print(\"Final loss: [{:.3f}, {:.3f}] Final accuracy: [{:.3f}, {:.3f}]\".format(\n",
        "                        loss_value, loss_value_validation, accuracy_value, accuracy_value_validation))\n",
        "  with tf.name_scope(\"evaluate\"):\n",
        "    # interpolate the predictive distributions and get the percentiles to represent\n",
        "    # an empirical credible interval for the predictions\n",
        "    predictions = np.asarray([sess.run(predictions,feed_dict={handle: heldout_handle})\n",
        "    for _ in range(FLAGS.num_monte_carlo)]):\n",
        "      predictions = np.squeeze(predictions) # fix the dimensions into a flat matrix\n",
        "      credible_intervals = [] # will be a matrix with with lower- and upper bound as columns\n",
        "      # loop over the columns and compute the empirical credible interval\n",
        "      modes = []\n",
        "    for i in range(predictions.shape[1]): \n",
        "      lb = np.percentile(predictions[: ,i], 2.5)\n",
        "      ub = np.percentile(predictions[: ,i], 97.5)\n",
        "      mode = np.mean(predictions[: ,i]\n",
        "      credible_intervals.append([lb, ub])\n",
        "      modes.append(mode)\n",
        "  # check how often the true value is inside the credible interval\n",
        "  with np.load(DATASET_FILE) as data:\n",
        "    labels = data[\"labels\"]\n",
        "    features = data[\"features\"]\n",
        "    train_range = int(TRAIN_PERCENTAGE * len(features))\n",
        "    validation_labels = labels[train_range:]\n",
        "    inside = 0\n",
        "    SSE = 0\n",
        "    for i in range(validation_labels.shape[0]):\n",
        "      label = validation_labels[i]\n",
        "      if label >= credible_intervals[i][0] and label <= credible_intervals[i][1]:\n",
        "        inside += 1\n",
        "        SSE += (label - modes[i] )**2\n",
        "  print(\"MSE\", SSE/validation_labels.shape[0])\n",
        "  print(inside/validation_labels.shape[0])\n",
        "\n",
        "tf.app.run()\n",
        "                                                    \n",
        "                                                    \n",
        "                \n",
        "     \n",
        "                                                    \n",
        "              \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "                                                    \n",
        "              \n",
        "        \n",
        "\n",
        "          \n",
        "          \n",
        "        \n",
        "      "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7915c7f4b60b>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    neural_net.add(layer)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_9TO9jAHONqB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}