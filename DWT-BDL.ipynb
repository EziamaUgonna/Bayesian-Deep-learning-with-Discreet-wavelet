{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  Dissertation2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian-Deep-learning-with-Discreet-wavelet/blob/master/Copy_of_Dissertation2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8RKLzCFYprQ",
        "outputId": "8172d596-b52a-456d-adb7-232cb5df1fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "import pywt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your datasets (assuming df1 contains the sensor values, df2 contains the ground truth)\n",
        "df1 = pd.read_csv('/content/drive/My Drive/Sensor1_values_bias_0_1_dur_10_dep.csv').astype(np.float32)\n",
        "df2 = pd.read_csv('/content/drive/My Drive/Ground_truth_bias_0_1_dur_10_sensor1_dep.csv')\n",
        "\n",
        "# Assuming df2 contains the ground truth (binary classification, 0 or 1)\n",
        "y = df2['ground_truth'].values  # Replace with the correct column name for ground truth\n",
        "X = df1.values  # Sensor readings as feature matrix\n",
        "\n",
        "# Discrete Wavelet Transform (DWT) for feature extraction\n",
        "def perform_dwt(data, wavelet='db4', level=3):\n",
        "    \"\"\"\n",
        "    Apply Discrete Wavelet Transform to the input data.\n",
        "    :param data: 1D signal array.\n",
        "    :param wavelet: Wavelet type (default: 'db4').\n",
        "    :param level: Decomposition level (default: 3).\n",
        "    :return: Concatenated wavelet coefficients as features.\n",
        "    \"\"\"\n",
        "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
        "    # Flatten the wavelet coefficients for input to NN\n",
        "    return np.hstack(coeffs)\n",
        "\n",
        "# Bayesian Dense Layer (Custom)\n",
        "class BayesianDenseLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Bayesian Dense layer using a normal distribution over the weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, prior_fn, posterior_fn, activation=None, **kwargs):\n",
        "        super(BayesianDenseLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.prior_fn = prior_fn\n",
        "        self.posterior_fn = posterior_fn\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize the weights and biases\n",
        "        self.w = self.add_weight(\n",
        "            name=\"weights\",\n",
        "            shape=(input_shape[1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            regularizer=None,\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            name=\"bias\",\n",
        "            shape=(self.units,),\n",
        "            initializer=\"zeros\",\n",
        "            regularizer=None,\n",
        "            trainable=True\n",
        "        )\n",
        "        super(BayesianDenseLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute the output using the learned weights\n",
        "        z = tf.matmul(inputs, self.w) + self.b\n",
        "        if self.activation:\n",
        "            z = self.activation(z)\n",
        "        return z\n",
        "\n",
        "# Helper functions for prior and posterior distributions\n",
        "def make_prior_fn():\n",
        "    \"\"\"Define the prior distribution as Normal.\"\"\"\n",
        "    tfd = tfp.distributions\n",
        "    return tfd.Normal(loc=0., scale=1.)\n",
        "\n",
        "def make_posterior_fn():\n",
        "    \"\"\"Define the posterior distribution as Normal with a softplus scale.\"\"\"\n",
        "    tfd = tfp.distributions\n",
        "    return lambda params: tfd.Normal(loc=params[..., :1], scale=tf.nn.softplus(params[..., 1:]))\n",
        "\n",
        "# Bayesian Neural Network Model\n",
        "class BayesianNN(Model):\n",
        "    \"\"\"\n",
        "    Custom Bayesian Neural Network that incorporates uncertainty over weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape, prior_fn, posterior_fn):\n",
        "        super(BayesianNN, self).__init__()\n",
        "        self.dense1 = BayesianDenseLayer(128, prior_fn, posterior_fn, activation=tf.nn.relu)\n",
        "        self.dense2 = BayesianDenseLayer(64, prior_fn, posterior_fn, activation=tf.nn.relu)\n",
        "        self.dense3 = BayesianDenseLayer(32, prior_fn, posterior_fn, activation=tf.nn.relu)\n",
        "        self.dense4 = BayesianDenseLayer(16, prior_fn, posterior_fn, activation=tf.nn.relu)\n",
        "        self.dense5 = BayesianDenseLayer(8, prior_fn, posterior_fn, activation=tf.nn.relu)\n",
        "        self.output_layer = BayesianDenseLayer(1, prior_fn, posterior_fn, activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        x = self.dense4(x)\n",
        "        x = self.dense5(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Data Preprocessing and DWT Extraction\n",
        "def preprocess_data(X, y, wavelet='db4', level=3):\n",
        "    \"\"\"\n",
        "    Apply DWT to input data and preprocess it for use in Bayesian Neural Network.\n",
        "    \"\"\"\n",
        "    # Apply DWT to each sample in the dataset\n",
        "    X_dwt = np.array([perform_dwt(x, wavelet, level) for x in X])\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_dwt)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Model Training and Evaluation\n",
        "def train_and_evaluate():\n",
        "    # Step 1: Generate and preprocess data\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
        "\n",
        "    # Step 2: Build the Bayesian Neural Network\n",
        "    input_shape = X_train.shape[1:]\n",
        "    prior_fn = make_prior_fn()\n",
        "    posterior_fn = make_posterior_fn()\n",
        "    model = BayesianNN(input_shape, prior_fn, posterior_fn)\n",
        "\n",
        "    # Compile the model with a loss function and optimizer\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Step 3: Train the model\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Step 4: Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # Step 5: Predict and analyze uncertainty\n",
        "    predictions = model(X_test)\n",
        "    print(\"Predictions:\", predictions.numpy())\n",
        "\n",
        "    # Plot the predicted values vs actual values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test, label='True Values', alpha=0.7)\n",
        "    plt.plot(predictions.numpy(), label='Predicted Values', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.title(\"True vs Predicted Values\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://ea464da4.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
