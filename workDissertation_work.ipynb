{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Dissertation_work.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian_analysis-/blob/master/workDissertation_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAfzZW_Kugw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "413ee375-d145-443e-ce13-434d663aeb47"
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: keras-preprocessing, astor, six, wrapt, opt-einsum, gast, termcolor, numpy, protobuf, grpcio, keras-applications, tensorflow-estimator, google-pasta, absl-py, tensorboard, wheel\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgEraww6ozqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e363b7b3-1712-4854-a840-9da1a681af73"
      },
      "source": [
        "!pip show tensorflow_probability"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow-probability\n",
            "Version: 0.7.0\n",
            "Summary: Probabilistic modeling and statistical inference in TensorFlow\n",
            "Home-page: http://github.com/tensorflow/probability\n",
            "Author: Google LLC\n",
            "Author-email: no-reply@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: decorator, numpy, cloudpickle, six\n",
            "Required-by: tensorflow-gan, tensor2tensor, magenta, kfac, dm-sonnet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUnoXqtYK636",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNaKznlPLZ1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "a9b9aa18-00a6-40c6-a701-be150a31f4e5"
      },
      "source": [
        "# Dependencies\n",
        "import os \n",
        "import warnings \n",
        "#from absl import flags \n",
        "import matplotlib \n",
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp\n",
        "import math\n",
        "import pandas as pd\n",
        "tfd = tfp.distributions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n",
        "#import python_utils"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sna8L90LzSy",
        "colab_type": "code",
        "outputId": "c28c51a0-5946-4234-8d57-9c9b9f49c006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "flags = tf.app.flags\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "flags.DEFINE_float(\"learning_rate\", default = 0.0001, help = \"Initial learning rate.\")\n",
        "flags.DEFINE_integer(\"epochs\", default = 700, help = \"Number of epochs to train for\")\n",
        "flags.DEFINE_integer(\"batch_size\", default =128, help = \"Batch size.\")\n",
        "flags.DEFINE_integer(\"eval_freq\", default = 400, help =\" Frequency at which to validate the model.\")\n",
        "flags.DEFINE_float(\"kernel_posterior_scale_mean\", default = -0.9, help = \"Initial kernel posterior mean of the scale (log var) for q(w)\")\n",
        "flags.DEFINE_float(\"kernel_posterior_scale_constraint\", default = 0.2, help = \"Posterior kernel constraint for the scale (log var) for q(w)\")\n",
        "flags.DEFINE_float(\"kl_annealing\", default = 50, help = \"Epochs to anneal the KL term (anneals from 0 to 1)\")\n",
        "flags.DEFINE_integer(\"num_hidden_layers\", default = 4, help = \"Number of hidden layers\")\n",
        "flags.DEFINE_integer(\"num_monte_carlo\",\n",
        "\n",
        "                     default=50,\n",
        "\n",
        "                     help=\"Network draws to compute predictive probabilities.\")\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "#initialize flags \n",
        "#FLAGS = flags.FLAGS\n",
        "print(FLAGS.learning_rate)\n",
        "print(FLAGS.epochs)\n",
        "print(FLAGS.num_monte_carlo)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0001\n",
            "700\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwkiDmUOUNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_pipeline(X_train,X_val,X_test,y_train,y_val,y_test, batch_size, valid_size):\n",
        "  #Build an iterator over training batches \n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  #Shuffle the dataset (note shuffle argument much larger than training size)\n",
        "  # and form batches of size batch_size\n",
        "  training_batches = training_dataset.shuffle(20000, reshuffle_each_iteration =True).repeat().batch(batch_size)\n",
        "  training_iterator = tf.data.make_one_shot_iterator(training_batches)\n",
        "  \n",
        "  #Building iterator over the heldout set with batch_size = heldout_size,\n",
        "  # i.e., return the entire heldout set as a constant.\n",
        "  heldout_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
        "  heldout_iterator = tf.data.make_one_shot_iterator(heldout_batches)\n",
        "  \n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
        "  test_dataset = test_dataset.batch(batch_size)\n",
        "  test_iterator = test_dataset.make_one_shot_iterator()\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  #Combine these into a feasible iterator that can switch between training \n",
        "  # and validation inputs.\n",
        "  # Here should be minibatch increment be defined \n",
        "  handle = tf.placeholder(tf.string, shape = [])\n",
        "  feedable_iterator = tf.data.Iterator.from_string_handle(handle, training_batches.output_types, training_batches.output_shapes)\n",
        "  features_final, labels_final = feedable_iterator.get_next()\n",
        "  \n",
        "  return features_final, labels_final, handle, training_iterator, heldout_iterator,test_iterator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qefGv6r-TEdJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bce8ed8-cb79-4e9e-d10b-1d1341d612da"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnqC6AdcTiDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7c3cdc7-8281-45ef-f4fd-80232627cc0e"
      },
      "source": [
        "!ls \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 01685ea0b1cbf3c8e71f60e0c86bd464.jpg\n",
            " 20151018_164402.jpg\n",
            " 20180702_052134.jpg\n",
            " 2d683da3d7a5c7b98eef15f5056bbd41.jpg\n",
            " 311096b0040456e8bdb94f5d9aabf0aa.jpg\n",
            " 400118_3456351547075_502332966_a.jpg\n",
            " 4f93ea67c48ebc72bd5f585ef349bb57.jpg\n",
            " 57d38f6f8c6cc4d993f3fd5dec581313.jpg\n",
            " 9e0f6a555af0ccfc820119a380e864bc.jpg\n",
            "'About Language. Tasks for teachers of English ( PDFDrive.com ).pdf'\n",
            "'a survey on trust management for interne of things.pdf'\n",
            "'Audio from Elvin Eziama'\n",
            "'Audio from Ugonna'\n",
            "'Blitzstein Hwang Probability.pdf'\n",
            "'Chinemezu '\n",
            "'Colab Notebooks'\n",
            "'Computational Physics - A Practical Introduction to Computational Physics and Scientific Computing ( PDFDrive.com ).pdf'\n",
            "'c++ primer.pdf'\n",
            " Dessert_code.ipynb\n",
            "'Dessert_codeV1.2 (1).ipynb'\n",
            " Dessert_codeV1.2.ipynb\n",
            " Dessert_codeV1F.2.ipynb\n",
            " Dropbox.exe\n",
            " foo.txt\n",
            "\"FORTUNATE'S TESTIMONIAL20001.jpg\"\n",
            " Franco.ipynb\n",
            "'Google Drive.1.pdf'\n",
            "'IJEOMA (2).JPG'\n",
            "'IJEOMA (3).jpg'\n",
            "'IJEOMA (5).JPG'\n",
            " IJEOMA.JPG\n",
            " IMG_20160828_011517.jpg\n",
            " IMG-20170520-WA000.jpg\n",
            " IMG-20180528-WA0002.jpg\n",
            " IMG_20180626_091658.jpg\n",
            " IMG-20180703-WA0001.jpg\n",
            " IMG-20180703-WA0003.jpg\n",
            " IMG-20180703-WA0004.jpg\n",
            " IMG_20191003_203100.jpg\n",
            " IMG_20191003_203128.jpg\n",
            " IMG_20191003_203155.jpg\n",
            " IMG_20191003_210402.jpg\n",
            " IMG_20191003_210429.jpg\n",
            " IMG_20191005_163702.jpg\n",
            " IMG_20191005_163716.jpg\n",
            " IMG_20191005_163731.jpg\n",
            " IMG_20191005_163755.jpg\n",
            " IMG_20191005_163831.jpg\n",
            " map_homo.ipynb\n",
            " NewShortcut2.exe\n",
            "'passport issue.pdf'\n",
            "'Photo from Elvin Eziama'\n",
            "'Photo from Elvin Eziama (1)'\n",
            "'Photo from Elvin Eziama (10)'\n",
            "'Photo from Elvin Eziama (11)'\n",
            "'Photo from Elvin Eziama (12)'\n",
            "'Photo from Elvin Eziama (13)'\n",
            "'Photo from Elvin Eziama (14)'\n",
            "'Photo from Elvin Eziama (15)'\n",
            "'Photo from Elvin Eziama (16)'\n",
            "'Photo from Elvin Eziama (17)'\n",
            "'Photo from Elvin Eziama (18)'\n",
            "'Photo from Elvin Eziama (19)'\n",
            "'Photo from Elvin Eziama (2)'\n",
            "'Photo from Elvin Eziama (20)'\n",
            "'Photo from Elvin Eziama (21)'\n",
            "'Photo from Elvin Eziama (22)'\n",
            "'Photo from Elvin Eziama (23)'\n",
            "'Photo from Elvin Eziama (24)'\n",
            "'Photo from Elvin Eziama (25)'\n",
            "'Photo from Elvin Eziama (26)'\n",
            "'Photo from Elvin Eziama (27)'\n",
            "'Photo from Elvin Eziama (28)'\n",
            "'Photo from Elvin Eziama (29)'\n",
            "'Photo from Elvin Eziama (3)'\n",
            "'Photo from Elvin Eziama (30)'\n",
            "'Photo from Elvin Eziama (31)'\n",
            "'Photo from Elvin Eziama (32)'\n",
            "'Photo from Elvin Eziama (33)'\n",
            "'Photo from Elvin Eziama (34)'\n",
            "'Photo from Elvin Eziama (35)'\n",
            "'Photo from Elvin Eziama (36)'\n",
            "'Photo from Elvin Eziama (37)'\n",
            "'Photo from Elvin Eziama (38)'\n",
            "'Photo from Elvin Eziama (39)'\n",
            "'Photo from Elvin Eziama (4)'\n",
            "'Photo from Elvin Eziama (40)'\n",
            "'Photo from Elvin Eziama (41)'\n",
            "'Photo from Elvin Eziama (42)'\n",
            "'Photo from Elvin Eziama (43)'\n",
            "'Photo from Elvin Eziama (44)'\n",
            "'Photo from Elvin Eziama (45)'\n",
            "'Photo from Elvin Eziama (46)'\n",
            "'Photo from Elvin Eziama (47)'\n",
            "'Photo from Elvin Eziama (5)'\n",
            "'Photo from Elvin Eziama (6)'\n",
            "'Photo from Elvin Eziama (7)'\n",
            "'Photo from Elvin Eziama (8)'\n",
            "'Photo from Elvin Eziama (9)'\n",
            "'Photo from Ugonna'\n",
            "'Photo from Ugonna (1)'\n",
            "'Photo from Ugonna (2)'\n",
            "'Photo from Ugonna (3)'\n",
            "'Photo from Ugonna (4)'\n",
            "'Photo from Ugonna (5)'\n",
            "'Photo from Ugonna (6)'\n",
            " Probabilistic_programming.pdf\n",
            "'Project data.zip'\n",
            "'project of a thing.txt'\n",
            "'Reconfigurable Computing.pdf'\n",
            " Results.gdoc\n",
            " R_inferno.pdf\n",
            "'Saneeha works.rar'\n",
            "'Screenshot_2018-06-26-20-45-43 (1).png'\n",
            " Screenshot_2018-06-26-20-45-43.png\n",
            " Screenshot_2018-06-26-20-54-15.png\n",
            " Screenshot_2018-06-26-20-54-29.png\n",
            "'Screenshot_2018-06-26-20-54-47 (1).png'\n",
            " Screenshot_2018-06-26-20-54-47.png\n",
            "'Screenshot_2018-06-26-20-55-56 (1).png'\n",
            " Screenshot_2018-06-26-20-55-56.png\n",
            "'Screenshot_2018-06-26-20-56-21 (1).png'\n",
            " Screenshot_2018-06-26-20-56-21.png\n",
            "'Screenshot_2018-06-26-20-56-42 (1).png'\n",
            " Screenshot_2018-06-26-20-56-42.png\n",
            "'Screenshot_2018-06-26-20-56-52 (1).png'\n",
            " Screenshot_2018-06-26-20-56-52.png\n",
            "'Screenshot_2018-06-26-20-57-09 (1).png'\n",
            " Screenshot_2018-06-26-20-57-09.png\n",
            "'Screenshot_2018-06-26-20-57-22 (1).png'\n",
            " Screenshot_2018-06-26-20-57-22.png\n",
            "'Screenshot_2018-06-26-20-58-27 (1).png'\n",
            " Screenshot_2018-06-26-20-58-27.png\n",
            "'Screenshot_2018-06-26-20-58-34 (1).png'\n",
            " Screenshot_2018-06-26-20-58-34.png\n",
            "'Screenshot_2018-06-26-20-58-42 (1).png'\n",
            " Screenshot_2018-06-26-20-58-42.png\n",
            "'Screenshot_2018-06-26-20-58-50 (1).png'\n",
            " Screenshot_2018-06-26-20-58-50.png\n",
            "'Screenshot_2018-06-26-20-58-55 (1).png'\n",
            " Screenshot_2018-06-26-20-58-55.png\n",
            "'Screenshot_2018-07-02-20-43-10 (1).png'\n",
            " Screenshot_2018-07-02-20-43-10.png\n",
            "'Screenshot_2018-07-02-20-43-17 (1).png'\n",
            " Screenshot_2018-07-02-20-43-17.png\n",
            " Screenshot_2018-08-16-00-33-50.png\n",
            " similar.m\n",
            "'Single anomalies.zip'\n",
            " Skype.exe\n",
            "'Sponsor Approval Request Form-1.pdf'\n",
            "'Sponsor Approval Request Form.pdf'\n",
            "'study permit.jpg'\n",
            "'ugonna passport and student permit.pdf'\n",
            " Untitled1.ipynb\n",
            " Untitled2.ipynb\n",
            " Untitled3.ipynb\n",
            " Untitled4.ipynb\n",
            " VID-20180407-WA0002.mp4\n",
            " VID-20180728-WA0000.mp4\n",
            " VID-20180728-WA0005.mp4\n",
            " VID-20180728-WA0009.mp4\n",
            "'Video from Elvin Eziama'\n",
            "'Video from Elvin Eziama (1)'\n",
            "'Video from Elvin Eziama (2)'\n",
            "'Video from Elvin Eziama (3)'\n",
            "'Video from Elvin Eziama (4)'\n",
            "'Video from Ugonna'\n",
            "'Video from Ugonna (1)'\n",
            "'Video from Ugonna (2)'\n",
            " wife\n",
            "'wife and kid documents.zip (Unzipped Files)'\n",
            "'winter letter of admission.pdf'\n",
            "'work2 (1).csv'\n",
            " work2.csv\n",
            " WP_20140122_013.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec33deBUSgOK",
        "colab_type": "code",
        "outputId": "1bad5d5b-7815-45e7-d939-ac8b1c03211b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read in the dataset\n",
        "df = pd.read_csv('/content/gdrive/My Drive/work2.csv')\n",
        "change = df.query('Speed>0').sample(frac = .1).index\n",
        "df.loc[change, 'Speed'] = 0\n",
        "df.loc[change, 'Class'] = 0\n",
        "df.to_csv('work2.csv', header = True, index =False)\n",
        "df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048575, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLK9IbUiUDU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:-1].values.astype(np.float32)\n",
        "y = df.iloc[:,-1].values.astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xUcWr4WNyH3",
        "colab_type": "code",
        "outputId": "8463a280-be7a-42bb-8cb4-c858764a98c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y.dtype"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5CMlzl6UEe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=200)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6md9xaxsYjRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the dataset  \n",
        "scalar_x_train = StandardScaler().fit(X_train)\n",
        "X_train = scalar_x_train.transform(X_train)\n",
        "X_test  = scalar_x_train.transform(X_test)\n",
        "X_val   = scalar_x_train.transform(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwg-fTyHH32k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c9559bef-851e-43ff-bfdb-6b0da9d614b0"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 13192611473223073644, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 16991940258924083465\n",
              " physical_device_desc: \"device: XLA_CPU device\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrhwLTTtbhrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#correct_prediction = tf.equal(prediction, tf.argmax(y_labels,1))\n",
        "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOLj-NVHa_Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(argv):\n",
        "  # extract the activation function from the hyperopt spec as an attribute from the tf.nn module \n",
        "  #activation = getattr(tf.nn, FLAGS.activation_function)\n",
        "  # define the graph \n",
        "  #with tf.Graph().as_default():\n",
        "  (features_final, labels_final, handle, training_iterator, heldout_iterator,test_iterator) = build_input_pipeline(X_train,X_val,X_test,y_train,y_val,y_test, FLAGS.batch_size, 500)\n",
        "  \n",
        "  \n",
        "  # Building the Bayesian Neural Network \n",
        "  # we are Gaussian Reparametrization Trick \n",
        "  # to compute the stochastic gradients as described in the paper \n",
        "  with tf.compat.v1.name_scope(\"bayesian_neural_net\", values =[features_final]):\n",
        "    neural_net = tf.keras.Sequential()\n",
        "    for i in range(FLAGS.num_hidden_layers):\n",
        "      layer = tfp.layers.DenseReparameterization(\n",
        "          units = 10,\n",
        "          activation = tf.nn.relu,\n",
        "          trainable = True,\n",
        "          kernel_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag\n",
        "          kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),\n",
        "          #kernel_posterior_fn=tfp_layers_util.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "          kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "          bias_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag\n",
        "          bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "          bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "          )\n",
        "      neural_net.add(layer)\n",
        "  neural_net.add(tfp.layers.DenseReparameterization(\n",
        "      units=2, # one dimensional output\n",
        "      activation= tf.nn.softmax, # since regression (outcome not bounded)\n",
        "      trainable=True, # i.e subject to optimization\n",
        "      kernel_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag with hyperopt sigma\n",
        "      kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "      kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "      bias_prior_fn =tfp.layers.default_multivariate_normal_fn, # NormalDiag with hyperopt sigma\n",
        "      bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "      bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "      ))\n",
        "  logits = neural_net(features_final)\n",
        "  #labels_distribution = tfd.Bernoulli(logits=logits)\n",
        "  labels_distribution = tfd.Categorical(logits=logits)\n",
        "  #labels_distribution = tfd.Bernoulli(logits=logits)\n",
        "  \n",
        "  # Perform KL annealing. The optimal number of annealing steps \n",
        "  # depends on the dataset and architecture.\n",
        "  t = tf.Variable(0.0)\n",
        "  kl_regularizer = t / (FLAGS.kl_annealing * len(X_train) / FLAGS.batch_size)\n",
        "  \n",
        "  #Compute the -ELBO as the loss. The kl term is annealed from 1 to 1 over \n",
        "  # the epochs specified by the kl_annealing flag.\n",
        "  log_likelihood = labels_distribution.log_prob(labels_final)\n",
        "  #neg_log_likelihood = tf.reduce_mean(tf.squared_difference(logits,labels_final))\n",
        "  neg_log_likelihood = -tf.reduce_mean(input_tensor = log_likelihood)\n",
        "  kl = sum(neural_net.losses)/len(X_train) * tf.minimum(1.0, kl_regularizer)\n",
        "  elbo_loss = neg_log_likelihood + kl\n",
        "  \n",
        "  # Build metrics for evaluation. Predictions are formed from single forward \n",
        "  # pass of the probablisitic layers . They are cheap but noisy predictions\n",
        "  predictions = tf.argmax(input = logits, axis=1)\n",
        "  #predictions = tf.cast(predictions, tf.int32)\n",
        "  # TP, TN, FP, FN\n",
        "  TP = tf.count_nonzero(predictions * labels_final)\n",
        "  TN = tf.count_nonzero((predictions - 1) * (labels_final - 1))\n",
        "  FP = tf.count_nonzero(predictions * (labels_final - 1))\n",
        "  FN = tf.count_nonzero((predictions - 1) * labels_final)\n",
        "  # precision, recall, f1\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1 = 2 * precision * recall / (precision + recall)\n",
        "  \n",
        "  tpr = TP/(TP+FN)\n",
        "  fpr = FP/(TP+FN)\n",
        "  with tf.compat.v1.name_scope(\"train\"):\n",
        "    correct_prediction = tf.equal(predictions, tf.argmax(labels_final))\n",
        "    training_accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "    \n",
        "    \n",
        "    #train_accuracy, train_accuracy_update_op = tf.metrics.accuracy(labels=labels_final,predictions =predictions)\n",
        "    opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
        "    train_op = opt.minimize(elbo_loss)\n",
        "    update_step_op = tf.assign(t, t+1)\n",
        "  \n",
        "  with tf.compat.v1.name_scope(\"valid\"):\n",
        "    correct_prediction_val = tf.equal(predictions, tf.argmax(labels_final))\n",
        "    valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction_val, tf.float32))\n",
        "    \n",
        "    #valid_accuracy, validation_accuracy_update_op = tf.metrics.accuracy(labels= labels_final,predictions = predictions)\n",
        "  with tf.compat.v1.name_scope(\"test\"):\n",
        "    \n",
        "    correct_prediction_test = tf.equal(predictions, tf.argmax(labels_final))\n",
        "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
        "    test_accuracy, test_accuracy_update_op = tf.metrics.accuracy(labels = labels_final,predictions = predictions)\n",
        "  \n",
        "  init_op = tf.group(tf.global_variables_initializer(),\n",
        "                     tf.local_variables_initializer())\n",
        "  saver = tf.train.Saver()\n",
        "  \n",
        " # stream_vars_valid = [ v for v in tf.local_variables() if \"valid\" in v.name]\n",
        "  \n",
        "  #reset_valid_op = tf.variables_initializer(stream_vars_valid)\n",
        "  \n",
        "  valid_accuracy_summary = []\n",
        "  stop_early =0\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    \n",
        "    # Run the training loop\n",
        "\n",
        "    train_handle = sess.run(training_iterator.string_handle())\n",
        "    heldout_handle = sess.run(  heldout_iterator.string_handle())\n",
        "      \n",
        "    test_handle = sess.run(test_iterator.string_handle())\n",
        "\n",
        "    training_steps = int(round(FLAGS.epochs * (len(X_train) / FLAGS.batch_size)))\n",
        "\n",
        "    for step in range(training_steps):\n",
        "      \n",
        "      _ = sess.run([train_op],feed_dict={handle: train_handle})\n",
        "\n",
        "     # _ = sess.run([train_op,train_accuracy_update_op, update_step_op],feed_dict={handle: train_handle})\n",
        "      \n",
        "      # Manually print the frequency \n",
        "      if step % 100 == 0:\n",
        "        save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
        "        loss_value, accuracy_value, kl_value = sess.run([elbo_loss, training_accuracy, kl], feed_dict= {handle:train_handle})\n",
        "        print(\"Step:{:>3d} loss : {:.3f} KL: {:.3f}\" .format(step , loss_value, accuracy_value, kl_value))\n",
        "        \n",
        "        \n",
        "      if (step +1) % FLAGS.eval_freq ==0: \n",
        "        # Compute log prob of heldout set by averaging draws from the model:\n",
        "        # p(heldout | train) = int_model p(heldout|model) p(model|train) ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
        "        # where model_i is a draw from the posterior \n",
        "        #p(model|train)\n",
        "        probs = np.asarray([sess.run((labels_distribution.probs), \n",
        "                                     feed_dict ={handle: heldout_handle})\n",
        "      \n",
        "                            for _ in range(FLAGS.num_monte_carlo)])\n",
        "        mean_probs = np.mean(probs, axis =0).astype(np.int32)\n",
        "        print(mean_probs.dtype)\n",
        "        \n",
        "        _, label_vals = sess.run((features_final, labels_final), feed_dict = {handle: heldout_handle})\n",
        "        #label_vals = (label_vals).astype(np.int32)\n",
        "      \n",
        "        heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]), label_vals]))\n",
        "        \n",
        "        \n",
        "        print(\" ...Held_out nats: {:.3f}\".format(heldout_lp))\n",
        "        \n",
        "       # Calculate validation accuracy\n",
        "\n",
        "        for step in range(10):\n",
        "\n",
        "          #sess.run(validation_accuracy_update_op, feed_dict={handle: heldout_handle})\n",
        "\n",
        "          valid_value = sess.run(valid_accuracy, feed_dict={handle: heldout_handle})\n",
        "        \n",
        "          valid_accuracy_summary.append(valid_value)\n",
        "          if valid_value < max(valid_accuracy_summary) and step > 100:\n",
        "            stop_early += 1\n",
        "          if stop_early == 15:\n",
        "            break\n",
        "          else:\n",
        "            stop_early = 0\n",
        "        print(\"Validation Accuracy: {:.3f}\".format(valid_value))\n",
        "        #sess.run(reset_valid_op)\n",
        "        \n",
        "        #sess.run(test_accuracy_update_op, feed_dict ={handle:test_handle})\n",
        "        #test_value = sess.run(test_accuracy, feed_dict= {handle:test_handle})\n",
        "          \n",
        "        test_value, _ = sess.run([test_accuracy, test_accuracy_update_op], feed_dict={handle: test_handle})\n",
        "        \n",
        "        loss_value,test_value, precision_value, recall_value, fpr_value, tpr_value,f1_value = sess.run([elbo_loss,test_accuracy, precision, recall, fpr, tpr,f1],feed_dict={handle: test_handle})\n",
        "        \n",
        "        print(\"Step: {:>3d} Loss: {:.3f}  test Accuracy: {:.3f} Precision: {:.3f} Recall: {:.3f} \".format(step, loss_value, test_value, precision_value, recall_value))\n",
        "        \n",
        "        print(\"Step: {:>3d} fpr: {:.3f} tpr: {:.3f} f1_1: {:.3f}\".format( step, fpr_value, tpr_value,f1_value))\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "  tf.compat.v1.app.run()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}