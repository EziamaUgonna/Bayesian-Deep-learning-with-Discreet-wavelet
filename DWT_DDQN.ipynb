{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DWT-DDQN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/DWT_BDL-and-DWT-DDQN/blob/master/DWT_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pywt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "GAMMA = 0.99  # Discount factor\n",
        "LEARNING_RATE = 0.0005\n",
        "BATCH_SIZE = 64\n",
        "MEMORY_SIZE = 100000\n",
        "TAU = 0.125  # For soft target updates\n",
        "EPSILON_DECAY = 0.995  # Epsilon decay for epsilon-greedy policy\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_START = 1.0\n",
        "EPISODES = 500\n",
        "MAX_STEPS = 200  # Maximum steps per episode\n",
        "\n",
        "# Load data\n",
        "df1 = pd.read_csv('/content/drive/My Drive/Sensor1_values_bias_0_1_dur_10_dep.csv').astype(np.float32)\n",
        "df2 = pd.read_csv('/content/drive/My Drive/Ground_truth_bias_0_1_dur_10_sensor1_dep.csv')\n",
        "\n",
        "# Assuming df2 contains the ground truth (binary classification, 0 or 1)\n",
        "y = df2['ground_truth'].values  # Replace with the correct column name for ground truth\n",
        "X = df1.values  # Sensor readings as feature matrix\n",
        "\n",
        "# Discrete Wavelet Transform for feature extraction\n",
        "def perform_dwt(data, wavelet='db4', level=3):\n",
        "    \"\"\"\n",
        "    Apply Discrete Wavelet Transform (DWT) to the input data.\n",
        "    :param data: 1D signal array.\n",
        "    :param wavelet: Wavelet type (default: 'db4').\n",
        "    :param level: Decomposition level (default: 3).\n",
        "    :return: Concatenated wavelet coefficients as features.\n",
        "    \"\"\"\n",
        "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
        "    return np.hstack(coeffs)  # Flatten coefficients for input to NN\n",
        "\n",
        "# Apply DWT to each row in the feature matrix X\n",
        "X_dwt = np.array([perform_dwt(x) for x in X])\n",
        "\n",
        "# Deep Q-Network (DQN) Model\n",
        "class DQN(Model):\n",
        "    def __init__(self, input_shape, action_space):\n",
        "        super(DQN, self).__init__()\n",
        "        self.dense1 = layers.Dense(256, activation='relu', input_shape=input_shape)\n",
        "        self.dense2 = layers.Dense(128, activation='relu')\n",
        "        self.dense3 = layers.Dense(64, activation='relu')\n",
        "        self.q_values = layers.Dense(action_space, activation='linear')  # Output Q-values for each action\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        return self.q_values(x)\n",
        "\n",
        "# Double DQN Agent\n",
        "class DDQNAgent:\n",
        "    def __init__(self, state_shape, action_space):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_space = action_space\n",
        "        self.epsilon = EPSILON_START\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "        # Initialize the DQN models\n",
        "        self.model = DQN(self.state_shape, self.action_space)\n",
        "        self.target_model = DQN(self.state_shape, self.action_space)\n",
        "        self.target_model.set_weights(self.model.get_weights())  # Initialize target model with the same weights\n",
        "\n",
        "        # Optimizer and loss function\n",
        "        self.optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "        self.loss_fn = tf.keras.losses.Huber()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # Soft update of target model (using tau)\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        model_weights = self.model.get_weights()\n",
        "        new_weights = [TAU * model_weight + (1 - TAU) * target_weight\n",
        "                       for model_weight, target_weight in zip(model_weights, target_weights)]\n",
        "        self.target_model.set_weights(new_weights)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(range(self.action_space))  # Explore\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        q_values = self.model(state)\n",
        "        return np.argmax(q_values[0].numpy())  # Exploit\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        # Sample a batch of experiences\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "\n",
        "        states = np.array([x[0] for x in batch])\n",
        "        actions = np.array([x[1] for x in batch])\n",
        "        rewards = np.array([x[2] for x in batch])\n",
        "        next_states = np.array([x[3] for x in batch])\n",
        "        dones = np.array([x[4] for x in batch])\n",
        "\n",
        "        # Q-value updates (Double DQN)\n",
        "        q_values_next = self.target_model(next_states)\n",
        "        q_values_online = self.model(next_states)\n",
        "\n",
        "        target_q_values = self.model(states)\n",
        "        for i in range(BATCH_SIZE):\n",
        "            if dones[i]:\n",
        "                target_q_values[i][actions[i]] = rewards[i]\n",
        "            else:\n",
        "                next_action = np.argmax(q_values_online[i])  # Greedy action from the online network\n",
        "                target_q_values[i][actions[i]] = rewards[i] + GAMMA * q_values_next[i][next_action]\n",
        "\n",
        "        # Compute loss and update weights\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values_pred = self.model(states)\n",
        "            loss = self.loss_fn(target_q_values, q_values_pred)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "        # Update epsilon for epsilon-greedy policy\n",
        "        if self.epsilon > EPSILON_MIN:\n",
        "            self.epsilon *= EPSILON_DECAY\n",
        "\n",
        "# Environment (Dummy)\n",
        "class DummyEnvironment:\n",
        "    def __init__(self, state_size, action_space):\n",
        "        self.state_size = state_size\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def reset(self):\n",
        "        return np.zeros(self.state_size)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = np.random.random(self.state_size)  # Random next state for example\n",
        "        reward = np.random.randn()  # Random reward\n",
        "        done = np.random.rand() > 0.95  # Random done condition\n",
        "        return next_state, reward, done\n",
        "\n",
        "# Training Loop\n",
        "def train_ddqn():\n",
        "    state_size = X_dwt.shape[1]  # Number of DWT features per sensor reading\n",
        "    action_space = 4  # Number of possible actions\n",
        "    agent = DDQNAgent(state_shape=(state_size,), action_space=action_space)\n",
        "    env = DummyEnvironment(state_size=state_size, action_space=action_space)\n",
        "\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(MAX_STEPS):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Store in memory\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            # Train the agent on a batch from memory\n",
        "            agent.replay()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update target model\n",
        "        agent.update_target_model()\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        print(f\"Episode {episode+1}/{EPISODES}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
        "\n",
        "        if episode % 50 == 0:\n",
        "            # Plotting episode rewards every 50 episodes\n",
        "            plt.plot(episode_rewards)\n",
        "            plt.title('Episode Rewards Over Time')\n",
        "            plt.show()\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    train_ddqn()\n"
      ],
      "metadata": {
        "id": "pV13tshBGuo3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}