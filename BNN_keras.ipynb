{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BNN_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian_analysis-/blob/master/BNN_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfAP82fA0orB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import tensorflow as tf \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "%matplotlib inline\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwyD3STE1N0J",
        "colab_type": "code",
        "outputId": "1165a49a-e368-43d8-d8c9-2cc93c9db2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_jzzx_u1Seh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCiIc7jl1WTp",
        "colab_type": "code",
        "outputId": "76bcccca-d4f1-4f07-e1cd-d386f8f8d8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read in the dataset\n",
        "df = pd.read_csv('/content/gdrive/My Drive/work2.csv').astype(np.float32)\n",
        "change = df.query('Speed>0').sample(frac = .1).index\n",
        "df.loc[change, 'Speed'] = 0\n",
        "df.loc[change, 'Class'] = 0\n",
        "df.to_csv('work2.csv', header = True, index =False)\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048575, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ibVf_rX1Way",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aANjA53f1WlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
        "#reshape y-data to become column vector \n",
        "y_train = np.reshape(y_train, [-1,1])\n",
        "y_test = np.reshape(y_test, [-1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPsanem11uKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the dataset \n",
        "scalar_x_train = StandardScaler().fit(X_train)\n",
        "scalar_x_test = StandardScaler().fit(X_test)\n",
        "X_train = scalar_x_train.transform(X_train)\n",
        "X_test = scalar_x_test.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqezAEkh1zaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18XtdbLdK_3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras import activations, initializers\n",
        "from keras.layers import Layer\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def mixture_prior_params(sigma_1, sigma_2, pi, return_sigma=False):\n",
        "    params = K.variable([sigma_1, sigma_2, pi], name='mixture_prior_params')\n",
        "    sigma = np.sqrt(pi * sigma_1 ** 2 + (1 - pi) * sigma_2 ** 2)\n",
        "    return params, sigma\n",
        "\n",
        "def log_mixture_prior_prob(w):\n",
        "    comp_1_dist = tf.distributions.Normal(0.0, prior_params[0])\n",
        "    comp_2_dist = tf.distributions.Normal(0.0, prior_params[1])\n",
        "    comp_1_weight = prior_params[2]    \n",
        "    return K.log(comp_1_weight * comp_1_dist.prob(w) + (1 - comp_1_weight) * comp_2_dist.prob(w))    \n",
        "\n",
        "# Mixture prior parameters shared across DenseVariational layer instances\n",
        "prior_params, prior_sigma = mixture_prior_params(sigma_1=1.0, sigma_2=0.1, pi=0.2)\n",
        "\n",
        "class DenseVariational(Layer):\n",
        "    def __init__(self, output_dim, kl_loss_weight, activation=None, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.kl_loss_weight = kl_loss_weight\n",
        "        self.activation = activations.get(activation)\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):  \n",
        "        self._trainable_weights.append(prior_params) \n",
        "\n",
        "        self.kernel_mu = self.add_weight(name='kernel_mu', \n",
        "                                         shape=(input_shape[1], self.output_dim),\n",
        "                                         initializer=initializers.normal(stddev=prior_sigma),\n",
        "                                         trainable=True)\n",
        "        self.bias_mu = self.add_weight(name='bias_mu', \n",
        "                                       shape=(self.output_dim,),\n",
        "                                       initializer=initializers.normal(stddev=prior_sigma),\n",
        "                                       trainable=True)\n",
        "        self.kernel_rho = self.add_weight(name='kernel_rho', \n",
        "                                          shape=(input_shape[1], self.output_dim),\n",
        "                                          initializer=initializers.constant(0.0),\n",
        "                                          trainable=True)\n",
        "        self.bias_rho = self.add_weight(name='bias_rho', \n",
        "                                        shape=(self.output_dim,),\n",
        "                                        initializer=initializers.constant(0.0),\n",
        "                                        trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        kernel = self.kernel_mu + kernel_sigma * tf.random.normal(self.kernel_mu.shape)\n",
        "\n",
        "        bias_sigma = tf.math.softplus(self.bias_rho)\n",
        "        bias = self.bias_mu + bias_sigma * tf.random.normal(self.bias_mu.shape)\n",
        "                \n",
        "        self.add_loss(self.kl_loss(kernel, self.kernel_mu, kernel_sigma) + \n",
        "                      self.kl_loss(bias, self.bias_mu, bias_sigma))\n",
        "        \n",
        "        return self.activation(K.dot(x, kernel) + bias)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)\n",
        "    \n",
        "    def kl_loss(self, w, mu, sigma):\n",
        "        variational_dist = tf.distributions.Normal(mu, sigma)\n",
        "        return kl_loss_weight * K.sum(variational_dist.log_prob(w) - log_mixture_prior_prob(w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYi14Y_44Qu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
        " \n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)   #Shuffle and the dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val,y_val))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "test_dataset =tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
        "test_dataset = test_dataset.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YZV7PB4Xqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = train_size\n",
        "num_batches = train_size / batch_size\n",
        "kl_loss_weight = 1.0 / num_batches\n",
        "\n",
        "x_in = Input(shape=(19,))\n",
        "x = DenseVariational(20, kl_loss_weight=kl_loss_weight, activation='relu')(x_in)\n",
        "x = DenseVariational(20, kl_loss_weight=kl_loss_weight, activation='relu')(x)\n",
        "x = DenseVariational(1, kl_loss_weight=kl_loss_weight)(x)\n",
        "model = Model(x_in, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAgWtgzh0Uos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =keras.Sequential([\n",
        "    DenseVariationall(20, kl_loss_weight=kl_loss_weight, activation='relu',input_shape=(19,))\n",
        "    DenseVariational(20, kl_loss_weight=kl_loss_weight, activation='relu'),\n",
        "    keras.laDenseVariationalyers.Dense(10, activation=tf.nn.softmax)\n",
        "])  DenseVariational(1, kl_loss_weight=kl_loss_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrFRmSB-04wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks, optimizers\n",
        "\n",
        "def neg_log_likelihood(y_test, y_pred, sigma=noise):\n",
        "    dist = tf.distributions.Normal(loc=y_pred, scale=sigma)\n",
        "    return K.sum(-dist.log_prob(y_true))\n",
        "\n",
        "model.compile(loss=neg_log_likelihood, optimizer=optimizers.Adam(lr=0.03), metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw2JF1qv6gZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_epochs = 1000\n",
        "\n",
        "#filepath = \"weight.best.hdf5\"\n",
        "#checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=0, save_best_only=True)\n",
        "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', min_delta=0.005, patience=20, verbose=0, restore_best_weights=True)\n",
        "reducelr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, verbose=1, min_lr=1e-7)\n",
        "model.fit(train_dataset,\n",
        "               epochs=max_epochs,steps_per_epoch=30,\n",
        "               shuffle=True,\n",
        "               verbose=2,\n",
        "               validation_data=val_dataset,validation_steps=3, callbacks = [earlystopper,reducelr] )\n",
        "\n",
        "\n",
        "#model.save(filepath)\n",
        "\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbTNIeb12DmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "\n",
        "#X_test = np.linspace(-1.5, 1.5, 1000).reshape(-1, 1)\n",
        "y_pred_list = []\n",
        "\n",
        "for i in tqdm.tqdm(range(500)):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_list.append(y_pred)\n",
        "    \n",
        "y_preds = np.concatenate(y_pred_list, axis=1)\n",
        "\n",
        "y_mean = np.mean(y_preds, axis=1)\n",
        "y_sigma = np.std(y_preds, axis=1)\n",
        "\n",
        "plt.plot(X_test, y_mean, 'r-', label='Predictive mean');\n",
        "plt.scatter(X, y, marker='+', label='Training data')\n",
        "plt.fill_between(X_test.ravel(), \n",
        "                 y_mean + 2 * y_sigma, \n",
        "                 y_mean - 2 * y_sigma, \n",
        "                 alpha=0.5, label='Epistemic uncertainty')\n",
        "plt.title('Prediction')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}