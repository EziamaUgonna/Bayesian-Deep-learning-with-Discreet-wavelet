{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dissertation_work.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian_analysis-/blob/master/Dissertation_BNN\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAfzZW_Kugw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgEraww6ozqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip show tensorflow_probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUnoXqtYK636",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNaKznlPLZ1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies\n",
        "import os \n",
        "import warnings \n",
        "#from absl import flags \n",
        "import matplotlib \n",
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp\n",
        "import math\n",
        "import pandas as pd\n",
        "tfd = tfp.distributions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n",
        "#import python_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sna8L90LzSy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "99228f06-d9f0-4d5a-cb9f-200ef57be9fc"
      },
      "source": [
        "flags = tf.app.flags\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "flags.DEFINE_float(\"learning_rate\", default = 0.0001, help = \"Initial learning rate.\")\n",
        "flags.DEFINE_integer(\"epochs\", default = 700, help = \"Number of epochs to train for\")\n",
        "flags.DEFINE_integer(\"batch_size\", default =128, help = \"Batch size.\")\n",
        "flags.DEFINE_integer(\"eval_freq\", default = 400, help =\" Frequency at which to validate the model.\")\n",
        "flags.DEFINE_float(\"kernel_posterior_scale_mean\", default = -0.9, help = \"Initial kernel posterior mean of the scale (log var) for q(w)\")\n",
        "flags.DEFINE_float(\"kernel_posterior_scale_constraint\", default = 0.2, help = \"Posterior kernel constraint for the scale (log var) for q(w)\")\n",
        "flags.DEFINE_float(\"kl_annealing\", default = 50, help = \"Epochs to anneal the KL term (anneals from 0 to 1)\")\n",
        "flags.DEFINE_integer(\"num_hidden_layers\", default = 4, help = \"Number of hidden layers\")\n",
        "flags.DEFINE_integer(\"num_monte_carlo\",\n",
        "\n",
        "                     default=50,\n",
        "\n",
        "                     help=\"Network draws to compute predictive probabilities.\")\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "#initialize flags \n",
        "#FLAGS = flags.FLAGS\n",
        "print(FLAGS.learning_rate)\n",
        "print(FLAGS.epochs)\n",
        "print(FLAGS.num_monte_carlo)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0001\n",
            "700\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwkiDmUOUNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_pipeline(X_train,X_test,y_train, y_test, batch_size, valid_size):\n",
        "  #Build an iterator over training batches \n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  #Shuffle the dataset (note shuffle argument much larger than training size)\n",
        "  # and form batches of size batch_size\n",
        "  training_batches = training_dataset.shuffle(20000, reshuffle_each_iteration =True).repeat().batch(batch_size)\n",
        "  training_iterator = tf.data.make_one_shot_iterator(training_batches)\n",
        "  \n",
        "  #Building iterator over the heldout set with batch_size = heldout_size,\n",
        "  # i.e., return the entire heldout set as a constant.\n",
        "  heldout_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
        "  heldout_iterator = tf.data.make_one_shot_iterator(heldout_batches)\n",
        "  \n",
        "  #Combine these into a feasible iterator that can switch between training \n",
        "  # and validation inputs.\n",
        "  # Here should be minibatch increment be defined \n",
        "  handle = tf.placeholder(tf.string, shape = [])\n",
        "  feedable_iterator = tf.data.Iterator.from_string_handle(handle, training_batches.output_types, training_batches.output_shapes)\n",
        "  features_final, labels_final = feedable_iterator.get_next()\n",
        "  \n",
        "  return features_final, labels_final, handle, training_iterator, heldout_iterator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qefGv6r-TEdJ",
        "colab_type": "code",
        "outputId": "ff74f022-9310-48f8-bb28-44cf6411fa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnqC6AdcTiDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec33deBUSgOK",
        "colab_type": "code",
        "outputId": "795b36c3-70cf-4026-a4d9-bc48254cd3a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read in the dataset\n",
        "df = pd.read_csv('/content/gdrive/My Drive/work2.csv').astype(np.float32)\n",
        "change = df.query('Speed>0').sample(frac = .1).index\n",
        "df.loc[change, 'Speed'] = 0\n",
        "df.loc[change, 'Class'] = 0\n",
        "df.to_csv('work2.csv', header = True, index =False)\n",
        "df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048575, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLK9IbUiUDU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xUcWr4WNyH3",
        "colab_type": "code",
        "outputId": "edc40e75-27b9-464e-e496-27ab94c47554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.dtype"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5CMlzl6UEe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =1)\n",
        "\n",
        "#reshape y-data to become column vector \n",
        "y_train = np.reshape(y_train, [-1,1])\n",
        "y_test = np.reshape(y_test, [-1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6md9xaxsYjRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the dataset \n",
        "scalar_x_train = StandardScaler().fit(X_train)\n",
        "scalar_x_test = StandardScaler().fit(X_test)\n",
        "X_train = scalar_x_train.transform(X_train)\n",
        "X_test = scalar_x_test.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwg-fTyHH32k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "80e2a713-e510-40f5-d138-aa8522f85ec0"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 7378803838222794496, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 18354752905183724801\n",
              " physical_device_desc: \"device: XLA_CPU device\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOLj-NVHa_Rg",
        "colab_type": "code",
        "outputId": "44351452-da12-4ee6-be00-7680fc672a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9078
        }
      },
      "source": [
        "def main(argv):\n",
        "  # extract the activation function from the hyperopt spec as an attribute from the tf.nn module \n",
        "  #activation = getattr(tf.nn, FLAGS.activation_function)\n",
        "  # define the graph \n",
        "  #with tf.Graph().as_default():\n",
        "  (features_final, labels_final, handle, training_iterator, heldout_iterator) = build_input_pipeline(X_train,X_test, y_train,y_test, FLAGS.batch_size, 500)\n",
        "  \n",
        "  \n",
        "  # Building the Bayesian Neural Network \n",
        "  # we are Gaussian Reparametrization Trick \n",
        "  # to compute the stochastic gradients as described in the paper \n",
        "  with tf.compat.v1.name_scope(\"bayesian_neural_net\", values =[features_final]):\n",
        "    neural_net = tf.keras.Sequential()\n",
        "    for i in range(FLAGS.num_hidden_layers):\n",
        "      layer = tfp.layers.DenseReparameterization(\n",
        "          units = 10,\n",
        "          activation = tf.nn.relu,\n",
        "          trainable = True,\n",
        "          kernel_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag\n",
        "          kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),\n",
        "          #kernel_posterior_fn=tfp_layers_util.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "          kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "          bias_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag\n",
        "          bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "          bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "          )\n",
        "      neural_net.add(layer)\n",
        "  neural_net.add(tfp.layers.DenseReparameterization(\n",
        "      units=2, # one dimensional output\n",
        "      activation= tf.nn.softmax, # since regression (outcome not bounded)\n",
        "      trainable=True, # i.e subject to optimization\n",
        "      kernel_prior_fn=tfp.layers.default_multivariate_normal_fn, # NormalDiag with hyperopt sigma\n",
        "      kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "      kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "      bias_prior_fn =tfp.layers.default_multivariate_normal_fn, # NormalDiag with hyperopt sigma\n",
        "      bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "      bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "      ))\n",
        "  logits = neural_net(features_final)\n",
        "  #labels_distribution = tfd.Bernoulli(logits=logits)\n",
        "  labels_distribution = tfd.Categorical(logits=logits)\n",
        "  #labels_distribution = tfd.Bernoulli(logits=logits)\n",
        "  \n",
        "  # Perform KL annealing. The optimal number of annealing steps \n",
        "  # depends on the dataset and architecture.\n",
        "  t = tf.Variable(0.0)\n",
        "  kl_regularizer = t / (FLAGS.kl_annealing * len(X_train) / FLAGS.batch_size)\n",
        "  \n",
        "  #Compute the -ELBO as the loss. The kl term is annealed from 1 to 1 over \n",
        "  # the epochs specified by the kl_annealing flag.\n",
        "  log_likelihood = labels_distribution.log_prob(labels_final)\n",
        "  #neg_log_likelihood = tf.reduce_mean(tf.squared_difference(logits,labels_final))\n",
        "  neg_log_likelihood = -tf.reduce_mean(input_tensor = log_likelihood)\n",
        "  kl = sum(neural_net.losses)/len(X_train) * tf.minimum(1.0, kl_regularizer)\n",
        "  elbo_loss = neg_log_likelihood + kl\n",
        "  \n",
        "  # Build metrics for evaluation. Predictions are formed from single forward \n",
        "  # pass of the probablisitic layers . They are cheap but noisy predictions\n",
        "  predictions = tf.argmax(input = logits, axis=1)\n",
        "  predictions = tf.cast(predictions, tf.float32)\n",
        "  # TP, TN, FP, FN\n",
        "  TP = tf.count_nonzero(predictions * labels_final)\n",
        "  TN = tf.count_nonzero((predictions - 1) * (labels_final - 1))\n",
        "  FP = tf.count_nonzero(predictions * (labels_final - 1))\n",
        "  FN = tf.count_nonzero((predictions - 1) * labels_final)\n",
        "  # precision, recall, f1\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1 = 2 * precision * recall / (precision + recall)\n",
        "  \n",
        "  tpr = TP/(TP+FN)\n",
        "  fpr = FP/(TP+FN)\n",
        "  with tf.compat.v1.name_scope(\"train\"):\n",
        "    train_accuracy, train_accuracy_update_op = tf.metrics.accuracy(labels=labels_final,predictions =predictions)\n",
        "    opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
        "    train_op = opt.minimize(elbo_loss)\n",
        "    update_step_op = tf.assign(t, t+1)\n",
        "  \n",
        "  with tf.compat.v1.name_scope(\"valid\"):\n",
        "    valid_accuracy, validation_accuracy_update_op = tf.metrics.accuracy(labels= labels_final,predictions = predictions)\n",
        "    \n",
        "  \n",
        "  init_op = tf.group(tf.global_variables_initializer(),\n",
        "                     tf.local_variables_initializer())\n",
        "  saver = tf.train.Saver()\n",
        "  \n",
        "  stream_vars_valid = [ v for v in tf.local_variables() if \"valid\" in v.name]\n",
        "  \n",
        "  reset_valid_op = tf.variables_initializer(stream_vars_valid)\n",
        "  \n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    \n",
        "    # Run the training loop\n",
        "\n",
        "    train_handle = sess.run(training_iterator.string_handle())\n",
        "\n",
        "    heldout_handle = sess.run(heldout_iterator.string_handle())\n",
        "\n",
        "    training_steps = int(\n",
        "\n",
        "        round(FLAGS.epochs * (len(X_train) / FLAGS.batch_size)))\n",
        "\n",
        "    for step in range(training_steps):\n",
        "      \n",
        "\n",
        "      _ = sess.run([train_op,train_accuracy_update_op, update_step_op],feed_dict={handle: train_handle})\n",
        "      \n",
        "      # Manually print the frequency \n",
        "      if step % 100 == 0:\n",
        "        save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
        "        loss_value, accuracy_value, kl_value = sess.run([elbo_loss, train_accuracy, kl], feed_dict= {handle:train_handle})\n",
        "        print(\"Step:{:>3d} loss : {:.3f} KL: {:.3f}\" .format(step , loss_value, accuracy_value, kl_value))\n",
        "        \n",
        "        \n",
        "      if (step +1) % FLAGS.eval_freq ==0: \n",
        "        # Compute log prob of heldout set by averaging draws from the model:\n",
        "        # p(heldout | train) = int_model p(heldout|model) p(model|train) ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
        "        # where model_i is a draw from the posterior \n",
        "        #p(model|train)\n",
        "        probs = np.asarray([sess.run((labels_distribution.probs), \n",
        "                                     feed_dict ={handle: heldout_handle})\n",
        "      \n",
        "                            for _ in range(FLAGS.num_monte_carlo)])\n",
        "        mean_probs = np.mean(probs, axis =0).astype(np.int32)\n",
        "        print(mean_probs.dtype)\n",
        "        \n",
        "        _, label_vals = sess.run((features_final, labels_final), feed_dict = {handle: heldout_handle})\n",
        "        label_vals = (label_vals).astype(np.int32)\n",
        "      \n",
        "        heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]), label_vals]))\n",
        "        \n",
        "        \n",
        "        print(\" ...Held_out nats: {:.3f}\".format(heldout_lp))\n",
        "        \n",
        "       # Calculate validation accuracy\n",
        "\n",
        "        for step in range(10):\n",
        "\n",
        "          sess.run(validation_accuracy_update_op, feed_dict={handle: heldout_handle})\n",
        "\n",
        "        valid_value = sess.run(valid_accuracy, feed_dict={handle: heldout_handle})\n",
        "\n",
        "        print(\n",
        "\n",
        "            \" ... Validation Accuracy: {:.3f}\".format(valid_value))\n",
        "        loss_value, precision_value, recall_value, fpr_value, tpr_value = sess.run([elbo_loss, precision, recall, fpr, tpr],feed_dict={handle: heldout_handle})\n",
        "        print(\"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f} Precision: {:.3f} Recall: {:.3f}\".format(\n",
        "            step, loss_value, accuracy_value, precision_value, recall_value))\n",
        "        print(\"Step: {:>3d} fpr: {:.3f} tpr: {:.3f} \".format(\n",
        "            step, fpr_value, tpr_value))\n",
        "\n",
        "        sess.run(reset_valid_op)\n",
        "if __name__ == \"__main__\": \n",
        "  tf.compat.v1.app.run()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:  0 loss : 0.682 KL: 0.922\n",
            "Step:100 loss : 0.662 KL: 0.856\n",
            "Step:200 loss : 0.679 KL: 0.878\n",
            "Step:300 loss : 0.692 KL: 0.888\n",
            "int32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:131: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.913\n",
            "Step:   9 Loss: 0.613 Accuracy: 0.888 Precision: 0.918 Recall: 1.000\n",
            "Step:   9 fpr: 0.089 tpr: 1.000 \n",
            "Step:400 loss : 0.603 KL: 0.893\n",
            "Step:500 loss : 0.589 KL: 0.896\n",
            "Step:600 loss : 0.507 KL: 0.898\n",
            "Step:700 loss : 0.492 KL: 0.899\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.909\n",
            "Step:   9 Loss: 0.440 Accuracy: 0.899 Precision: 0.912 Recall: 1.000\n",
            "Step:   9 fpr: 0.096 tpr: 1.000 \n",
            "Step:800 loss : 0.453 KL: 0.900\n",
            "Step:900 loss : 0.440 KL: 0.901\n",
            "Step:1000 loss : 0.461 KL: 0.901\n",
            "Step:1100 loss : 0.400 KL: 0.902\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.901\n",
            "Step:   9 Loss: 0.417 Accuracy: 0.902 Precision: 0.904 Recall: 1.000\n",
            "Step:   9 fpr: 0.106 tpr: 1.000 \n",
            "Step:1200 loss : 0.409 KL: 0.902\n",
            "Step:1300 loss : 0.435 KL: 0.903\n",
            "Step:1400 loss : 0.441 KL: 0.903\n",
            "Step:1500 loss : 0.365 KL: 0.903\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.413 Accuracy: 0.903 Precision: 0.902 Recall: 1.000\n",
            "Step:   9 fpr: 0.109 tpr: 1.000 \n",
            "Step:1600 loss : 0.370 KL: 0.904\n",
            "Step:1700 loss : 0.370 KL: 0.904\n",
            "Step:1800 loss : 0.440 KL: 0.904\n",
            "Step:1900 loss : 0.439 KL: 0.904\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.372 Accuracy: 0.904 Precision: 0.942 Recall: 1.000\n",
            "Step:   9 fpr: 0.062 tpr: 1.000 \n",
            "Step:2000 loss : 0.423 KL: 0.905\n",
            "Step:2100 loss : 0.439 KL: 0.905\n",
            "Step:2200 loss : 0.368 KL: 0.905\n",
            "Step:2300 loss : 0.439 KL: 0.905\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.387 Accuracy: 0.905 Precision: 0.926 Recall: 1.000\n",
            "Step:   9 fpr: 0.080 tpr: 1.000 \n",
            "Step:2400 loss : 0.392 KL: 0.905\n",
            "Step:2500 loss : 0.423 KL: 0.905\n",
            "Step:2600 loss : 0.454 KL: 0.906\n",
            "Step:2700 loss : 0.384 KL: 0.906\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.904\n",
            "Step:   9 Loss: 0.414 Accuracy: 0.906 Precision: 0.900 Recall: 1.000\n",
            "Step:   9 fpr: 0.111 tpr: 1.000 \n",
            "Step:2800 loss : 0.423 KL: 0.906\n",
            "Step:2900 loss : 0.415 KL: 0.906\n",
            "Step:3000 loss : 0.384 KL: 0.906\n",
            "Step:3100 loss : 0.454 KL: 0.906\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.897\n",
            "Step:   9 Loss: 0.428 Accuracy: 0.906 Precision: 0.886 Recall: 1.000\n",
            "Step:   9 fpr: 0.129 tpr: 1.000 \n",
            "Step:3200 loss : 0.391 KL: 0.906\n",
            "Step:3300 loss : 0.391 KL: 0.906\n",
            "Step:3400 loss : 0.399 KL: 0.906\n",
            "Step:3500 loss : 0.391 KL: 0.906\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.907\n",
            "Step:   9 Loss: 0.411 Accuracy: 0.906 Precision: 0.902 Recall: 1.000\n",
            "Step:   9 fpr: 0.109 tpr: 1.000 \n",
            "Step:3600 loss : 0.399 KL: 0.907\n",
            "Step:3700 loss : 0.423 KL: 0.906\n",
            "Step:3800 loss : 0.407 KL: 0.907\n",
            "Step:3900 loss : 0.391 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.911\n",
            "Step:   9 Loss: 0.417 Accuracy: 0.907 Precision: 0.896 Recall: 1.000\n",
            "Step:   9 fpr: 0.116 tpr: 1.000 \n",
            "Step:4000 loss : 0.407 KL: 0.907\n",
            "Step:4100 loss : 0.423 KL: 0.907\n",
            "Step:4200 loss : 0.430 KL: 0.907\n",
            "Step:4300 loss : 0.423 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.907\n",
            "Step:   9 Loss: 0.387 Accuracy: 0.907 Precision: 0.926 Recall: 1.000\n",
            "Step:   9 fpr: 0.080 tpr: 1.000 \n",
            "Step:4400 loss : 0.391 KL: 0.907\n",
            "Step:4500 loss : 0.423 KL: 0.907\n",
            "Step:4600 loss : 0.391 KL: 0.907\n",
            "Step:4700 loss : 0.415 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.913\n",
            "Step:   9 Loss: 0.409 Accuracy: 0.907 Precision: 0.904 Recall: 1.000\n",
            "Step:   9 fpr: 0.106 tpr: 1.000 \n",
            "Step:4800 loss : 0.423 KL: 0.907\n",
            "Step:4900 loss : 0.415 KL: 0.907\n",
            "Step:5000 loss : 0.415 KL: 0.907\n",
            "Step:5100 loss : 0.391 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.915\n",
            "Step:   9 Loss: 0.411 Accuracy: 0.907 Precision: 0.902 Recall: 1.000\n",
            "Step:   9 fpr: 0.109 tpr: 1.000 \n",
            "Step:5200 loss : 0.384 KL: 0.907\n",
            "Step:5300 loss : 0.384 KL: 0.907\n",
            "Step:5400 loss : 0.423 KL: 0.907\n",
            "Step:5500 loss : 0.438 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.912\n",
            "Step:   9 Loss: 0.397 Accuracy: 0.907 Precision: 0.916 Recall: 1.000\n",
            "Step:   9 fpr: 0.092 tpr: 1.000 \n",
            "Step:5600 loss : 0.423 KL: 0.907\n",
            "Step:5700 loss : 0.360 KL: 0.907\n",
            "Step:5800 loss : 0.431 KL: 0.907\n",
            "Step:5900 loss : 0.384 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.910\n",
            "Step:   9 Loss: 0.411 Accuracy: 0.907 Precision: 0.902 Recall: 1.000\n",
            "Step:   9 fpr: 0.109 tpr: 1.000 \n",
            "Step:6000 loss : 0.415 KL: 0.907\n",
            "Step:6100 loss : 0.360 KL: 0.907\n",
            "Step:6200 loss : 0.368 KL: 0.907\n",
            "Step:6300 loss : 0.430 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.903\n",
            "Step:   9 Loss: 0.393 Accuracy: 0.907 Precision: 0.920 Recall: 1.000\n",
            "Step:   9 fpr: 0.087 tpr: 1.000 \n",
            "Step:6400 loss : 0.376 KL: 0.907\n",
            "Step:6500 loss : 0.438 KL: 0.907\n",
            "Step:6600 loss : 0.415 KL: 0.907\n",
            "Step:6700 loss : 0.423 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.914\n",
            "Step:   9 Loss: 0.391 Accuracy: 0.907 Precision: 0.922 Recall: 1.000\n",
            "Step:   9 fpr: 0.085 tpr: 1.000 \n",
            "Step:6800 loss : 0.407 KL: 0.907\n",
            "Step:6900 loss : 0.399 KL: 0.907\n",
            "Step:7000 loss : 0.384 KL: 0.907\n",
            "Step:7100 loss : 0.368 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.909\n",
            "Step:   9 Loss: 0.391 Accuracy: 0.907 Precision: 0.922 Recall: 1.000\n",
            "Step:   9 fpr: 0.085 tpr: 1.000 \n",
            "Step:7200 loss : 0.407 KL: 0.907\n",
            "Step:7300 loss : 0.384 KL: 0.907\n",
            "Step:7400 loss : 0.423 KL: 0.907\n",
            "Step:7500 loss : 0.470 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.409 Accuracy: 0.907 Precision: 0.904 Recall: 1.000\n",
            "Step:   9 fpr: 0.106 tpr: 1.000 \n",
            "Step:7600 loss : 0.384 KL: 0.907\n",
            "Step:7700 loss : 0.384 KL: 0.907\n",
            "Step:7800 loss : 0.399 KL: 0.907\n",
            "Step:7900 loss : 0.415 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.907\n",
            "Step:   9 Loss: 0.393 Accuracy: 0.907 Precision: 0.920 Recall: 1.000\n",
            "Step:   9 fpr: 0.087 tpr: 1.000 \n",
            "Step:8000 loss : 0.376 KL: 0.907\n",
            "Step:8100 loss : 0.423 KL: 0.907\n",
            "Step:8200 loss : 0.399 KL: 0.907\n",
            "Step:8300 loss : 0.423 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.904\n",
            "Step:   9 Loss: 0.395 Accuracy: 0.907 Precision: 0.918 Recall: 1.000\n",
            "Step:   9 fpr: 0.089 tpr: 1.000 \n",
            "Step:8400 loss : 0.430 KL: 0.907\n",
            "Step:8500 loss : 0.454 KL: 0.907\n",
            "Step:8600 loss : 0.415 KL: 0.907\n",
            "Step:8700 loss : 0.391 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.415 Accuracy: 0.907 Precision: 0.898 Recall: 1.000\n",
            "Step:   9 fpr: 0.114 tpr: 1.000 \n",
            "Step:8800 loss : 0.399 KL: 0.907\n",
            "Step:8900 loss : 0.470 KL: 0.907\n",
            "Step:9000 loss : 0.438 KL: 0.907\n",
            "Step:9100 loss : 0.399 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.903\n",
            "Step:   9 Loss: 0.377 Accuracy: 0.907 Precision: 0.936 Recall: 1.000\n",
            "Step:   9 fpr: 0.068 tpr: 1.000 \n",
            "Step:9200 loss : 0.391 KL: 0.907\n",
            "Step:9300 loss : 0.391 KL: 0.907\n",
            "Step:9400 loss : 0.384 KL: 0.907\n",
            "Step:9500 loss : 0.407 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.907\n",
            "Step:   9 Loss: 0.415 Accuracy: 0.907 Precision: 0.898 Recall: 1.000\n",
            "Step:   9 fpr: 0.114 tpr: 1.000 \n",
            "Step:9600 loss : 0.360 KL: 0.907\n",
            "Step:9700 loss : 0.407 KL: 0.907\n",
            "Step:9800 loss : 0.423 KL: 0.907\n",
            "Step:9900 loss : 0.446 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.905\n",
            "Step:   9 Loss: 0.421 Accuracy: 0.907 Precision: 0.892 Recall: 1.000\n",
            "Step:   9 fpr: 0.121 tpr: 1.000 \n",
            "Step:10000 loss : 0.399 KL: 0.907\n",
            "Step:10100 loss : 0.384 KL: 0.908\n",
            "Step:10200 loss : 0.384 KL: 0.908\n",
            "Step:10300 loss : 0.431 KL: 0.907\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.411 Accuracy: 0.907 Precision: 0.902 Recall: 1.000\n",
            "Step:   9 fpr: 0.109 tpr: 1.000 \n",
            "Step:10400 loss : 0.399 KL: 0.908\n",
            "Step:10500 loss : 0.368 KL: 0.908\n",
            "Step:10600 loss : 0.391 KL: 0.908\n",
            "Step:10700 loss : 0.431 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.907\n",
            "Step:   9 Loss: 0.407 Accuracy: 0.908 Precision: 0.906 Recall: 1.000\n",
            "Step:   9 fpr: 0.104 tpr: 1.000 \n",
            "Step:10800 loss : 0.391 KL: 0.908\n",
            "Step:10900 loss : 0.462 KL: 0.908\n",
            "Step:11000 loss : 0.352 KL: 0.908\n",
            "Step:11100 loss : 0.415 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.909\n",
            "Step:   9 Loss: 0.393 Accuracy: 0.908 Precision: 0.920 Recall: 1.000\n",
            "Step:   9 fpr: 0.087 tpr: 1.000 \n",
            "Step:11200 loss : 0.391 KL: 0.908\n",
            "Step:11300 loss : 0.407 KL: 0.908\n",
            "Step:11400 loss : 0.391 KL: 0.908\n",
            "Step:11500 loss : 0.399 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.904\n",
            "Step:   9 Loss: 0.391 Accuracy: 0.908 Precision: 0.922 Recall: 1.000\n",
            "Step:   9 fpr: 0.085 tpr: 1.000 \n",
            "Step:11600 loss : 0.391 KL: 0.908\n",
            "Step:11700 loss : 0.368 KL: 0.908\n",
            "Step:11800 loss : 0.407 KL: 0.908\n",
            "Step:11900 loss : 0.399 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.913\n",
            "Step:   9 Loss: 0.429 Accuracy: 0.908 Precision: 0.884 Recall: 1.000\n",
            "Step:   9 fpr: 0.131 tpr: 1.000 \n",
            "Step:12000 loss : 0.399 KL: 0.908\n",
            "Step:12100 loss : 0.454 KL: 0.908\n",
            "Step:12200 loss : 0.376 KL: 0.908\n",
            "Step:12300 loss : 0.391 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.421 Accuracy: 0.908 Precision: 0.892 Recall: 1.000\n",
            "Step:   9 fpr: 0.121 tpr: 1.000 \n",
            "Step:12400 loss : 0.399 KL: 0.908\n",
            "Step:12500 loss : 0.423 KL: 0.908\n",
            "Step:12600 loss : 0.431 KL: 0.908\n",
            "Step:12700 loss : 0.431 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.908\n",
            "Step:   9 Loss: 0.437 Accuracy: 0.908 Precision: 0.876 Recall: 1.000\n",
            "Step:   9 fpr: 0.142 tpr: 1.000 \n",
            "Step:12800 loss : 0.438 KL: 0.908\n",
            "Step:12900 loss : 0.431 KL: 0.908\n",
            "Step:13000 loss : 0.438 KL: 0.908\n",
            "Step:13100 loss : 0.368 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.904\n",
            "Step:   9 Loss: 0.405 Accuracy: 0.908 Precision: 0.908 Recall: 1.000\n",
            "Step:   9 fpr: 0.101 tpr: 1.000 \n",
            "Step:13200 loss : 0.438 KL: 0.908\n",
            "Step:13300 loss : 0.384 KL: 0.908\n",
            "Step:13400 loss : 0.368 KL: 0.908\n",
            "Step:13500 loss : 0.446 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.904\n",
            "Step:   9 Loss: 0.423 Accuracy: 0.908 Precision: 0.890 Recall: 1.000\n",
            "Step:   9 fpr: 0.124 tpr: 1.000 \n",
            "Step:13600 loss : 0.454 KL: 0.908\n",
            "Step:13700 loss : 0.376 KL: 0.908\n",
            "Step:13800 loss : 0.415 KL: 0.908\n",
            "Step:13900 loss : 0.407 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.903\n",
            "Step:   9 Loss: 0.425 Accuracy: 0.908 Precision: 0.888 Recall: 1.000\n",
            "Step:   9 fpr: 0.126 tpr: 1.000 \n",
            "Step:14000 loss : 0.407 KL: 0.908\n",
            "Step:14100 loss : 0.399 KL: 0.908\n",
            "Step:14200 loss : 0.423 KL: 0.908\n",
            "Step:14300 loss : 0.391 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.908\n",
            "Step:   9 Loss: 0.417 Accuracy: 0.908 Precision: 0.896 Recall: 1.000\n",
            "Step:   9 fpr: 0.116 tpr: 1.000 \n",
            "Step:14400 loss : 0.407 KL: 0.908\n",
            "Step:14500 loss : 0.423 KL: 0.908\n",
            "Step:14600 loss : 0.360 KL: 0.908\n",
            "Step:14700 loss : 0.391 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.914\n",
            "Step:   9 Loss: 0.397 Accuracy: 0.908 Precision: 0.916 Recall: 1.000\n",
            "Step:   9 fpr: 0.092 tpr: 1.000 \n",
            "Step:14800 loss : 0.391 KL: 0.908\n",
            "Step:14900 loss : 0.407 KL: 0.908\n",
            "Step:15000 loss : 0.454 KL: 0.908\n",
            "Step:15100 loss : 0.438 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.910\n",
            "Step:   9 Loss: 0.399 Accuracy: 0.908 Precision: 0.914 Recall: 1.000\n",
            "Step:   9 fpr: 0.094 tpr: 1.000 \n",
            "Step:15200 loss : 0.423 KL: 0.908\n",
            "Step:15300 loss : 0.360 KL: 0.908\n",
            "Step:15400 loss : 0.391 KL: 0.908\n",
            "Step:15500 loss : 0.391 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.910\n",
            "Step:   9 Loss: 0.385 Accuracy: 0.908 Precision: 0.928 Recall: 1.000\n",
            "Step:   9 fpr: 0.078 tpr: 1.000 \n",
            "Step:15600 loss : 0.399 KL: 0.908\n",
            "Step:15700 loss : 0.399 KL: 0.908\n",
            "Step:15800 loss : 0.337 KL: 0.908\n",
            "Step:15900 loss : 0.391 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.911\n",
            "Step:   9 Loss: 0.387 Accuracy: 0.908 Precision: 0.926 Recall: 1.000\n",
            "Step:   9 fpr: 0.080 tpr: 1.000 \n",
            "Step:16000 loss : 0.384 KL: 0.908\n",
            "Step:16100 loss : 0.415 KL: 0.908\n",
            "Step:16200 loss : 0.376 KL: 0.908\n",
            "Step:16300 loss : 0.423 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.913\n",
            "Step:   9 Loss: 0.415 Accuracy: 0.908 Precision: 0.898 Recall: 1.000\n",
            "Step:   9 fpr: 0.114 tpr: 1.000 \n",
            "Step:16400 loss : 0.399 KL: 0.908\n",
            "Step:16500 loss : 0.462 KL: 0.908\n",
            "Step:16600 loss : 0.415 KL: 0.908\n",
            "Step:16700 loss : 0.352 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.910\n",
            "Step:   9 Loss: 0.399 Accuracy: 0.908 Precision: 0.914 Recall: 1.000\n",
            "Step:   9 fpr: 0.094 tpr: 1.000 \n",
            "Step:16800 loss : 0.384 KL: 0.908\n",
            "Step:16900 loss : 0.415 KL: 0.908\n",
            "Step:17000 loss : 0.477 KL: 0.908\n",
            "Step:17100 loss : 0.470 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.419 Accuracy: 0.908 Precision: 0.894 Recall: 1.000\n",
            "Step:   9 fpr: 0.119 tpr: 1.000 \n",
            "Step:17200 loss : 0.368 KL: 0.908\n",
            "Step:17300 loss : 0.391 KL: 0.908\n",
            "Step:17400 loss : 0.384 KL: 0.908\n",
            "Step:17500 loss : 0.431 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.907\n",
            "Step:   9 Loss: 0.389 Accuracy: 0.908 Precision: 0.924 Recall: 1.000\n",
            "Step:   9 fpr: 0.082 tpr: 1.000 \n",
            "Step:17600 loss : 0.384 KL: 0.908\n",
            "Step:17700 loss : 0.391 KL: 0.908\n",
            "Step:17800 loss : 0.415 KL: 0.908\n",
            "Step:17900 loss : 0.407 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.912\n",
            "Step:   9 Loss: 0.409 Accuracy: 0.908 Precision: 0.904 Recall: 1.000\n",
            "Step:   9 fpr: 0.106 tpr: 1.000 \n",
            "Step:18000 loss : 0.438 KL: 0.908\n",
            "Step:18100 loss : 0.438 KL: 0.908\n",
            "Step:18200 loss : 0.399 KL: 0.908\n",
            "Step:18300 loss : 0.399 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.409 Accuracy: 0.908 Precision: 0.904 Recall: 1.000\n",
            "Step:   9 fpr: 0.106 tpr: 1.000 \n",
            "Step:18400 loss : 0.407 KL: 0.908\n",
            "Step:18500 loss : 0.431 KL: 0.908\n",
            "Step:18600 loss : 0.384 KL: 0.908\n",
            "Step:18700 loss : 0.438 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.912\n",
            "Step:   9 Loss: 0.393 Accuracy: 0.908 Precision: 0.920 Recall: 1.000\n",
            "Step:   9 fpr: 0.087 tpr: 1.000 \n",
            "Step:18800 loss : 0.431 KL: 0.908\n",
            "Step:18900 loss : 0.446 KL: 0.908\n",
            "Step:19000 loss : 0.399 KL: 0.908\n",
            "Step:19100 loss : 0.407 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.909\n",
            "Step:   9 Loss: 0.429 Accuracy: 0.908 Precision: 0.884 Recall: 1.000\n",
            "Step:   9 fpr: 0.131 tpr: 1.000 \n",
            "Step:19200 loss : 0.454 KL: 0.908\n",
            "Step:19300 loss : 0.423 KL: 0.908\n",
            "Step:19400 loss : 0.407 KL: 0.908\n",
            "Step:19500 loss : 0.407 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.900\n",
            "Step:   9 Loss: 0.405 Accuracy: 0.908 Precision: 0.908 Recall: 1.000\n",
            "Step:   9 fpr: 0.101 tpr: 1.000 \n",
            "Step:19600 loss : 0.462 KL: 0.908\n",
            "Step:19700 loss : 0.415 KL: 0.908\n",
            "Step:19800 loss : 0.391 KL: 0.908\n",
            "Step:19900 loss : 0.407 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.904\n",
            "Step:   9 Loss: 0.397 Accuracy: 0.908 Precision: 0.916 Recall: 1.000\n",
            "Step:   9 fpr: 0.092 tpr: 1.000 \n",
            "Step:20000 loss : 0.384 KL: 0.908\n",
            "Step:20100 loss : 0.407 KL: 0.908\n",
            "Step:20200 loss : 0.391 KL: 0.908\n",
            "Step:20300 loss : 0.446 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.908\n",
            "Step:   9 Loss: 0.407 Accuracy: 0.908 Precision: 0.906 Recall: 1.000\n",
            "Step:   9 fpr: 0.104 tpr: 1.000 \n",
            "Step:20400 loss : 0.391 KL: 0.908\n",
            "Step:20500 loss : 0.391 KL: 0.908\n",
            "Step:20600 loss : 0.399 KL: 0.908\n",
            "Step:20700 loss : 0.391 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.912\n",
            "Step:   9 Loss: 0.415 Accuracy: 0.908 Precision: 0.898 Recall: 1.000\n",
            "Step:   9 fpr: 0.114 tpr: 1.000 \n",
            "Step:20800 loss : 0.423 KL: 0.908\n",
            "Step:20900 loss : 0.454 KL: 0.908\n",
            "Step:21000 loss : 0.376 KL: 0.908\n",
            "Step:21100 loss : 0.368 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.910\n",
            "Step:   9 Loss: 0.403 Accuracy: 0.908 Precision: 0.910 Recall: 1.000\n",
            "Step:   9 fpr: 0.099 tpr: 1.000 \n",
            "Step:21200 loss : 0.438 KL: 0.908\n",
            "Step:21300 loss : 0.376 KL: 0.908\n",
            "Step:21400 loss : 0.407 KL: 0.908\n",
            "Step:21500 loss : 0.399 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.908\n",
            "Step:   9 Loss: 0.417 Accuracy: 0.908 Precision: 0.896 Recall: 1.000\n",
            "Step:   9 fpr: 0.116 tpr: 1.000 \n",
            "Step:21600 loss : 0.376 KL: 0.908\n",
            "Step:21700 loss : 0.399 KL: 0.908\n",
            "Step:21800 loss : 0.407 KL: 0.908\n",
            "Step:21900 loss : 0.431 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.910\n",
            "Step:   9 Loss: 0.393 Accuracy: 0.908 Precision: 0.920 Recall: 1.000\n",
            "Step:   9 fpr: 0.087 tpr: 1.000 \n",
            "Step:22000 loss : 0.431 KL: 0.908\n",
            "Step:22100 loss : 0.399 KL: 0.908\n",
            "Step:22200 loss : 0.446 KL: 0.908\n",
            "Step:22300 loss : 0.399 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.904\n",
            "Step:   9 Loss: 0.391 Accuracy: 0.908 Precision: 0.922 Recall: 1.000\n",
            "Step:   9 fpr: 0.085 tpr: 1.000 \n",
            "Step:22400 loss : 0.384 KL: 0.908\n",
            "Step:22500 loss : 0.438 KL: 0.908\n",
            "Step:22600 loss : 0.368 KL: 0.908\n",
            "Step:22700 loss : 0.399 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.912\n",
            "Step:   9 Loss: 0.431 Accuracy: 0.908 Precision: 0.882 Recall: 1.000\n",
            "Step:   9 fpr: 0.134 tpr: 1.000 \n",
            "Step:22800 loss : 0.470 KL: 0.908\n",
            "Step:22900 loss : 0.337 KL: 0.908\n",
            "Step:23000 loss : 0.431 KL: 0.908\n",
            "Step:23100 loss : 0.391 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.407 Accuracy: 0.908 Precision: 0.906 Recall: 1.000\n",
            "Step:   9 fpr: 0.104 tpr: 1.000 \n",
            "Step:23200 loss : 0.423 KL: 0.908\n",
            "Step:23300 loss : 0.360 KL: 0.908\n",
            "Step:23400 loss : 0.376 KL: 0.908\n",
            "Step:23500 loss : 0.376 KL: 0.908\n",
            "int32\n",
            " ...Held_out nats: -inf\n",
            " ... Validation Accuracy: 0.906\n",
            "Step:   9 Loss: 0.399 Accuracy: 0.908 Precision: 0.914 Recall: 1.000\n",
            "Step:   9 fpr: 0.094 tpr: 1.000 \n",
            "Step:23600 loss : 0.423 KL: 0.908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsDbKB8s6y1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the training datasets\n",
        "dx_train = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "# apply a one-hot transformation to each label for use in the neural network\n",
        "dy_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "# zip the x and y training data together and shuffle, batch etc.\n",
        "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqE__LXp8O2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do the same operations for the validation set\n",
        "dx_valid = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "dy_valid = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15H6bFJr8YBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create general iterator\n",
        "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
        "                                               train_dataset.output_shapes)\n",
        "next_element = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "950KHnXX86nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make datasets that we can initialize separately, but using the same structure via the common iterator\n",
        "training_init_op = iterator.make_initializer(train_dataset)\n",
        "validation_init_op = iterator.make_initializer(valid_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bnpSRzD8-Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_model(in_data):\n",
        "    bn = tf.layers.batch_normalization(in_data)\n",
        "    fc1 = tf.layers.dense(bn, 19)\n",
        "    fc2 = tf.layers.dense(fc1, 16)\n",
        "    fc2 = tf.layers.dense(fc2, 14)\n",
        "    fc3 = tf.layers.dense(fc2, 2)\n",
        "    return fc3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htk6gugl9CF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "cb9be9e2-8c82-4e5a-e7fe-d215aa00db15"
      },
      "source": [
        "# create the neural network model\n",
        "logits = nn_model(next_element[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0620 02:09:36.389599 139678856390528 deprecation.py:323] From <ipython-input-20-00433fe29e41>:2: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W0620 02:09:36.498937 139678856390528 deprecation.py:323] From <ipython-input-20-00433fe29e41>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0620 02:09:36.505683 139678856390528 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5dH11Qw9JBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the optimizer and loss\n",
        "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(1e-7).minimize(loss)\n",
        "# get accuracy\n",
        "predictions = tf.argmax(logits, 1)\n",
        "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "\n",
        "\n",
        "#TP, TN, FP, FN\n",
        "TP = tf.count_nonzero(predictions * y_test, dtype=tf.float32)\n",
        "TN = tf.count_nonzero((predictions-1)*(y_test-1), dtype=tf.float32)\n",
        "FP = tf.count_nonzero(predictions*(y_test-1), dtype=tf.float32)\n",
        "FN = tf.count_nonzero((predictions-1)*y_test, dtype=tf.float32)\n",
        "#precision, recall, f1\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1 = 2 * precision * precision * recall / (precision + recall)\n",
        "tpr = TP/(TP+FN)\n",
        "fpr = FP/(TP+FN)\n",
        "\n",
        "init_op = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahp2boNx9MyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run the training\n",
        "epochs = 600\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    sess.run(training_init_op)\n",
        "    for i in range(epochs):\n",
        "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
        "        if i % 50 == 0:\n",
        "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
        "    # now setup the validation run\n",
        "    valid_iters = 1000\n",
        "    # re-initialize the iterator, but this time with validation data\n",
        "    sess.run(validation_init_op)\n",
        "    avg_acc = 0\n",
        "    for i in range(valid_iters):\n",
        "        \n",
        "        loss_value, valid_accuracy, precision_value, recall_value, fpr_value, tpr_value,f1_value = sess.run([loss, accuracy, precision, recall, fpr, tpr,f1])\n",
        "        print(\"Step: {:>3d} loss: {:.3f} Validation Accuracy: {:.3f} Precision: {:.3f} Recall: {:.3f} fpr:{:.3f} tpr:{:.3f} f1:{:.3f} \".format(\n",
        "            i, loss_value, valid_accuracy, precision_value, recall_value, fpr_value, tpr_value, f1_value))\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        acc = sess.run([accuracy])\n",
        "        avg_acc += acc[0]\n",
        "        print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters,(avg_acc ) ))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_gMBuw_9Wnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,  f1_score\n",
        "\n",
        "#clf = svm.SVC(kernel='rbf',degree=3, probability=False,tol=0.001)\n",
        "#clf = RandomForestClassifier(n_estimators=100,random_state=0)\n",
        "clf = LogisticRegression(C=0.1)\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "fpr, tpr = roc_curve(y_test,y_pred)\n",
        "print(confusion_matrix(y_test,y_pred))  \n",
        "print(classification_report(y_test,y_pred))  \n",
        "print(precision_score(y_test,y_pred))\n",
        "print( recall_score(y_test,y_pred))\n",
        "print( f1_score(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(fpr, tpr)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}