{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fixed BNN_Work.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian_analysis-/blob/master/Fixed_BNN_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KeocMQyXDUhK",
        "colab_type": "code",
        "outputId": "4a9cec83-829d-4168-fa13-5b2e7915b1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q gspread\n",
        "from tensorboardcolab import *\n",
        "import shutil\n",
        "#clean out the directory\n",
        "shutil.rmtree('./Graph', ignore_errors=True)\n",
        "os.mkdir('./Graph')\n",
        "tf.reset_default_graph()\n",
        "#will start the tunneling and will print out a link:\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://207f5966.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tb8KDqWJOmoi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gc\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VyhfJ9g7Q-cF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/csamuelsson/bayesianNN\n",
        "!mv bayesianNN/* .\n",
        "!rm -rf bayesianNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ns2O-ymR42v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YLOC26ZEpr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "from flags import *\n",
        "#from absl import flags\n",
        "import python_utils\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OaMY_bVVEtmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RhPmxemmE00z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_worksheet(filename, sheetname):\n",
        "    #read from Google Sheets\n",
        "    rows = gc.open(filename).worksheet(sheetname).get_all_values()\n",
        "    return rows\n",
        "\n",
        "def read_worksheets(filename, sheetname, col_names):\n",
        "    datasets = []\n",
        "    for sheet in sheetname:\n",
        "        rows = read_worksheet(filename,sheet)\n",
        "        df = pd.DataFrame.from_records(rows,columns=col_names )\n",
        "        datasets.append( df )\n",
        "    return datasets\n",
        "        \n",
        "\n",
        "def merge_datasets(datasets):\n",
        "    base = datasets[0]\n",
        "    for ds in datasets[1:]:\n",
        "        #print(len(ds))\n",
        "        base = base.append(ds,ignore_index=True)\n",
        "    return base\n",
        "\n",
        "#def splitting_data(dataset, ratio_1 = 0.8, ratio_2 = 0.9, shuffle= True, seed = 0):\n",
        "    #split_1 = int(ratio_1*len(datasets))\n",
        "   # split_2 = int(ratio_2*len(datasets))\n",
        "    #if shuffle:\n",
        "        #np.random.seed(seed = seed)\n",
        "       # np.random.shuffle(datasets.values)\n",
        "    #train = datasets[:split_1]\n",
        "    #validation= datasets[split_1:split_2]\n",
        "    #test = datasets[split_2:]\n",
        "    #return train, validation, test\n",
        "\n",
        "#def split_dataset(dataset,ratio=0.3,shuffle=True,seed=0):\n",
        "    #cnt = len(dataset)\n",
        "    #cut = math.ceil(ratio*cnt)\n",
        "    \n",
        "    #if shuffle:\n",
        "       # np.random.seed(seed=seed)\n",
        "       # np.random.shuffle(dataset.values)\n",
        "        \n",
        "   # valid = dataset[:cut]\n",
        "    #train = dataset[cut:]\n",
        "    #return train,valid\n",
        "\n",
        "def normalize(df,column):\n",
        "    values = df[column].astype(float).values\n",
        "    mean = np.mean(values)\n",
        "    s = np.sum(values)\n",
        "    std = np.std(values)\n",
        "    norm_values = ( values - mean ) / std\n",
        "    df[column] = norm_values\n",
        "    return mean,std\n",
        "\n",
        "\n",
        "\n",
        "def normalize_columns(df,col_names):\n",
        "    means_stds = {}\n",
        "    for col in col_names:\n",
        "        means_stds[col] = normalize(df,col)\n",
        "    return means_stds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MfZ1Kc1aE2kL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#filename = 'data1.csv'\n",
        "filename = 'dataset-baysian'\n",
        "columns = 'columns'\n",
        "sheets = ['data2','data23','data59','data6']\n",
        "\n",
        "\n",
        "local_file = Path(filename+\".csv\")\n",
        "\n",
        "unified = None\n",
        "\n",
        "if not local_file.exists():\n",
        "    col_names = read_worksheet(filename,columns)[0]\n",
        "    datasets = read_worksheets(filename,sheets,col_names)\n",
        "    #display( *[df.head() for df in datasets] )\n",
        "    unified = merge_datasets(datasets)\n",
        "    unified.to_csv(local_file)\n",
        "else:\n",
        "    unified = pd.read_csv(local_file)\n",
        "labels = unified['class']\n",
        "features = unified.drop(['class'], axis=1, inplace=False)\n",
        "features \n",
        "labels "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RxnsDcFE9Uy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# \"class\" != 1 => 0\n",
        "unified['class'].replace(to_replace=r'[^1]+', value='0', inplace=True, regex=True)\n",
        "labels = unified['class']\n",
        "#norm_cols = ['time', 'messageID', 'pos/0', 'pos/1', 'pos/2', 'spd/0', 'spd/1']\n",
        "#means_stds = normalize_columns(unified,norm_cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCVrt7UTE-r7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.savez(\"data_main.npz\", features=unified.values.astype(np.float32), labels=labels.values.astype(np.float32))\n",
        "\n",
        "unified.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pCBgEn_3FEJT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfd = tf.contrib.distributions\n",
        "\n",
        "\"\"\"Tuning program settings\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "FLAGS.learning_rate = 0.09 # change\n",
        "FLAGS.num_hidden_layers = 7\n",
        "FLAGS.num_neurons_per_layer = 3\n",
        "FLAGS.activation_function = \"sigmoid\"\n",
        "FLAGS.num_principal_components = 7\n",
        "FLAGS.batch_size = 1000     # kept constant under hyperopt\n",
        "FLAGS.num_epochs = 3  # kept constant under hyperopt\n",
        "TRAIN_PERCENTAGE = 0.8\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "USE_PCA = False\n",
        "FLAGS = flags.FLAGS\n",
        "TRAIN_PERCENTAGE = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1iqbbc4FJhp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_input_pipeline(batch_size, number_of_principal_components):\n",
        "    \"\"\"Build an Iterator switching between train and heldout data.\n",
        "    Args:\n",
        "    `data_main.npz`: string representing the path to the .npz dataset.\n",
        "    `batch_size`: integer specifying the batch_size for the dataset.\n",
        "    `number_of_principal_components`: integer specifying how many principal components\n",
        "    to reduce the dataset into.\n",
        "    \"\"\"\n",
        "    # Build an iterator over training batches.\n",
        "    with np.load('data_main.npz') as DATASET_FILE:\n",
        "        features = DATASET_FILE[\"features\"]\n",
        "        labels = DATASET_FILE[\"labels\"]\n",
        "        features = features.astype(np.float32)\n",
        "        labels = labels.astype(np.float32)\n",
        "        \n",
        "        # PCA (sklearn)\n",
        "        if USE_PCA:\n",
        "            features = PCA(n_components=number_of_principal_components).fit_transform(features)\n",
        "        \n",
        "        # Splitting into training and validation sets\n",
        "        #train_range = int(TRAIN_PERCENTAGE * len(features))\n",
        "        \n",
        "        split_1 = int(TRAIN_PERCENTAGE*len(features))\n",
        "        split_2 = int(TRAIN_PERCENTAGE+0.1 *len(features))\n",
        "        \n",
        "        training_features = features[:split_1]\n",
        "        training_labels = labels[: split_1]\n",
        "        validation_features = features[ split_1: split_2]\n",
        "        validation_labels = labels[split_1:split_2]\n",
        "        testing_features = features[split_2:]\n",
        "        testing_labels      = labels[split_2:]\n",
        "        \n",
        "        # Z-normalising: (note with respect to training data)\n",
        "        training_features = (training_features - np.mean(training_features, axis=0))/np.std(training_features, axis=0)\n",
        "        validation_features = (validation_features - np.mean(training_features, axis=0))/np.std(training_features, axis=0)\n",
        "        testing_features    = (testing_features -np.mean(training_features, axis=0))/np.std(training_features, axis=0)\n",
        "    # Create the tf.Dataset object\n",
        "    training_dataset = tf.data.Dataset.from_tensor_slices((training_features, training_labels))\n",
        "    \n",
        "    # Shuffle the dataset (note shuffle argument much larger than training size)\n",
        "    # and form batches of size `batch_size`\n",
        "    training_batches = training_dataset.shuffle(20000).repeat().batch(batch_size)\n",
        "    training_iterator = training_batches.make_one_shot_iterator()\n",
        "    \n",
        "    # Build a iterator over the heldout set with batch_size=heldout_size,\n",
        "    # i.e., return the entire heldout set as a constant.\n",
        "    heldout_dataset = tf.data.Dataset.from_tensor_slices((validation_features, validation_labels))\n",
        "    heldout_frozen = (heldout_dataset.take(len(validation_features)).\n",
        "                    repeat().batch(len(validation_features)))\n",
        "    heldout_iterator = heldout_frozen.make_one_shot_iterator()\n",
        "    \n",
        "    \n",
        "    #Building iterator over the testing set \n",
        "    testing_dataset = tf.data.Dataset.from_tensor_slices((testing_features, testing_labels))\n",
        "    testing_batches = testing_dataset.shuffle(20000).repeat().batch(batch_size)\n",
        "    testing_iterator = training_batches.make_one_shot_iterator()\n",
        "    \n",
        "    \n",
        "    # Combine these into a feedable iterator that can switch between training\n",
        "    # validation,and testing inputs.\n",
        "    # Here should the minibatch increment be defined \n",
        "    handle = tf.placeholder(tf.string, shape=[],)\n",
        "    feedable_iterator = tf.data.Iterator.from_string_handle(\n",
        "    handle, training_batches.output_types, training_batches.output_shapes)\n",
        "    features_final, labels_final = feedable_iterator.get_next()\n",
        "    \n",
        "    return features_final, labels_final, handle, training_iterator, heldout_iterator, testing_iterator, split_1, split_2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XwMDiQDTJZcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(argv):\n",
        "    # extract the activation function from the hyperopt spec as an attribute from the tf.nn module\n",
        "    activation = getattr(tf.nn, FLAGS.activation_function)\n",
        "    # define the graph\n",
        "    with tf.Graph().as_default():\n",
        "        (features, labels, handle,\n",
        "         training_iterator, heldout_iterator, testing_iterator, train_range, valid_range) = build_input_pipeline(\n",
        "            FLAGS.batch_size, FLAGS.num_principal_components)\n",
        "        # Building the Bayesian Neural Network. \n",
        "        # We are here using the Gaussian Reparametrization Trick\n",
        "        # to compute the stochastic gradients as described in the paper\n",
        "        with tf.name_scope(\"bayesian_neural_net\", values=[features]):\n",
        "            neural_net = tf.keras.Sequential()\n",
        "            for i in range(FLAGS.num_hidden_layers):\n",
        "                layer = tfp.layers.DenseReparameterization(\n",
        "                    units=FLAGS.num_neurons_per_layer,\n",
        "                    activation=activation,\n",
        "                    trainable=True,\n",
        "                    kernel_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
        "                    # NormalDiag with hyperopt sigmakernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # softplus(sigma)\n",
        "                    kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),  # softplus(sigma)\n",
        "                    kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "                    bias_prior_fn=tfp.layers.default_multivariate_normal_fn,  # NormalDiag with hyperopt sigma\n",
        "                    bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(),  # softplus(sigma)\n",
        "                    bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "                )\n",
        "                neural_net.add(layer)\n",
        "            neural_net.add(tfp.layers.DenseReparameterization(\n",
        "                units=1,  # one dimensional output\n",
        "                activation=None,  # since regression (outcome not bounded)\n",
        "                trainable=True,  # i.e subject to optimization\n",
        "                kernel_prior_fn=tfp.layers.default_multivariate_normal_fn,  # NormalDiag\n",
        "                kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),  # softplus(sigma)\n",
        "                kernel_posterior_tensor_fn=lambda x: x.sample(),\n",
        "                bias_prior_fn=tfp.layers.default_multivariate_normal_fn,  # NormalDiag\n",
        "                bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(),  # softplus(sigma)\n",
        "                bias_posterior_tensor_fn=lambda x: x.sample()\n",
        "            ))\n",
        "            \n",
        "        predictions = neural_net(features)\n",
        "        preds = []\n",
        "        for _ in range(1000):\n",
        "            preds.append(neural_net(features))\n",
        "            MAP, var = tf.nn.moments(tf.squeeze(preds), axes=[0])\n",
        "            \n",
        "        # Compute the -ELBO as the loss, averaged over the batch size.\n",
        "        neg_log_likelihood = tf.reduce_mean(tf.squared_difference(labels, predictions))\n",
        "        kl = sum(neural_net.losses) / FLAGS.batch_size\n",
        "        elbo_loss = kl + neg_log_likelihood\n",
        "        # Build metrics for evaluation. Predictions are formed from a single forward\n",
        "        # pass of the probabilistic layers. They are cheap but noisy predictions.\n",
        "        accuracy, accuracy_update_op = tf.nn.softmax_cross_entropy_with_logits_v2(labels,predictions)\n",
        "\n",
        "        with tf.name_scope(\"train\"):\n",
        "            # define optimizer - we are using (stochastic) gradient descent\n",
        "            opt = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "            # define that we want to minimize the loss (-ELBO)\n",
        "            train_op = opt.minimize(elbo_loss)\n",
        "            # start the session\n",
        "            sess = tf.Session()\n",
        "            # initialize the variables\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            sess.run(tf.local_variables_initializer())\n",
        "\n",
        "            # Run the training loop\n",
        "            train_handle = sess.run(training_iterator.string_handle())\n",
        "            heldout_handle = sess.run(heldout_iterator.string_handle())\n",
        "            test_handle = sess.run(test_iterator.string_handle())\n",
        "            # Run the epochs\n",
        "            for epoch in range(FLAGS.num_epochs):\n",
        "                _ = sess.run([train_op, accuracy_update_op],\n",
        "                             feed_dict={handle: train_handle})\n",
        "\n",
        "                if epoch % 100 == 0:\n",
        "                    loss_value, accuracy_value = sess.run(\n",
        "                        [elbo_loss, accuracy], feed_dict={handle: train_handle})\n",
        "                    loss_value_validation, accuracy_value_validation = sess.run(\n",
        "                        [elbo_loss, accuracy], feed_dict={handle: heldout_handle}\n",
        "                    )\n",
        "                    print(\"Epoch: {:>3d} Loss: [{:.3f}, {:.3f}] Accuracy: [{:.3f}, {:.3f}]\".format(\n",
        "                        epoch, loss_value, loss_value_validation, accuracy_value, accuracy_value_validation))\n",
        "\n",
        "                # Check if final epoch, if so return the validation loss for the last epoch             \n",
        "                if epoch == FLAGS.num_epochs - 1:\n",
        "                    final_loss, final_accuracy = sess.run(\n",
        "                        [elbo_loss, accuracy], feed_dict={handle: heldout_handle}\n",
        "                    )\n",
        "                    print(\"Final loss: [{:.3f}, {:.3f}] Final accuracy: [{:.3f}, {:.3f}]\".format(\n",
        "                        loss_value, loss_value_validation, accuracy_value, accuracy_value_validation))\n",
        "        with tf.name_scope(\"evaluate\"):\n",
        "            # interpolate the predictive distributions and get the percentiles to represent\n",
        "            # an empirical credible interval for the predictions\n",
        "            probs = np.asarray([sess.run((labels_distribution.probs),\n",
        "                                         feed_dict={handle: heldout_handle})\n",
        "                                for _ in range(FLAGS.num_monte_carlo)])\n",
        "            predictions = np.squeeze(predictions)  # fix the dimensions into a flat matrix\n",
        "            credible_intervals = []  # will be a matrix with with lower- and upper bound as columns\n",
        "            # loop over the columns and compute the empirical credible interval\n",
        "            modes = []\n",
        "            for i in range(predictions.shape[1]):\n",
        "                lb = np.percentile(predictions[:, i], 2.5)\n",
        "                ub = np.percentile(predictions[:, i], 97.5)\n",
        "                mode = np.mean(predictions[:, i])\n",
        "                credible_intervals.append([lb, ub])\n",
        "                modes.append(mode)\n",
        "    # check how often the true value is inside the credible interval\n",
        "    with load('data_main.npz') as data:\n",
        "        print(data)\n",
        "        labels = data[\"labels\"]\n",
        "        features = data[\"features\"]\n",
        "        train_range = int(TRAIN_PERCENTAGE * len(features))\n",
        "        validation_labels = labels[train_range:]\n",
        "        inside = 0\n",
        "        SSE = 0\n",
        "        for i in range(validation_labels.shape[0]):\n",
        "            label = validation_labels[i]\n",
        "            if label >= credible_intervals[i][0] and label <= credible_intervals[i][1]:\n",
        "                inside += 1\n",
        "                SSE += (label - modes[i]) ** 2\n",
        "    print(\"MSE\", SSE / validation_labels.shape[0])\n",
        "    print(inside / validation_labels.shape[0])\n",
        "\n",
        "# HYPERPARAMETER OPTIMIZATION\n",
        "\n",
        "# Define the hyperparametric space (some form of prior by specyfying range)\n",
        "fspace = {\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.0001, 0.1),\n",
        "    'num_hidden_layers': hp.uniform('num_hidden_layers', 1, 7 + 1),\n",
        "    'num_neurons_per_layer': hp.uniform('num_neurons_per_layer', 5, 200 + 1),\n",
        "    'activation_function': hp.choice('activation_function', [\"sigmoid\", \"relu\"]),\n",
        "    'num_principal_components': hp.uniform('num_principal_components', 20, 500 + 1)\n",
        "}\n",
        "\n",
        "# Wrapper around the objective function, assigns the flag values from the trials\n",
        "def wrapper(params):\n",
        "    FLAGS.learning_rate = params['learning_rate']\n",
        "    FLAGS.num_hidden_layers = int(params['num_hidden_layers'])\n",
        "    FLAGS.num_neurons_per_layer = int(params['num_neurons_per_layer'])\n",
        "    FLAGS.activation_function = params['activation_function']\n",
        "    FLAGS.num_principal_components = int(params['num_principal_components'])\n",
        "    FLAGS.batch_size = 44  # kept constant\n",
        "    FLAGS.num_epochs = 10000  # kept constant\n",
        "    FLAGS.num_monte_carlo = 50  # default value \n",
        "    return main(FLAGS)\n",
        "\n",
        "def caller(argv):\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=wrapper, space=fspace, algo=tpe.suggest, max_evals=1000, trials=trials)\n",
        "    print(\"Best:\", best)\n",
        "    print(\"Trials:\")\n",
        "    for trial in trials.trials:\n",
        "        print(trial)\n",
        "\n",
        "\n",
        "tf.app.run(main=caller)\n",
        "# tf.app.run()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}