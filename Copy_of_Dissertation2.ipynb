{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  Dissertation2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EziamaUgonna/Bayesian_analysis-/blob/master/Copy_of_Dissertation2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8RKLzCFYprQ",
        "colab_type": "code",
        "outputId": "ab1d3d06-12a9-4d36-9fd4-4c7be92cfeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!pip install --upgrade -q gspread\n",
        "from tensorboardcolab import *\n",
        "import shutil\n",
        "#clean out the directory\n",
        "shutil.rmtree('./Graph', ignore_errors=True)\n",
        "os.mkdir('./Graph')\n",
        "#tf.reset_default_graph()\n",
        "#will start the tunneling and will print out a link:\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://f1f9964f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmaXvvyQYyU0",
        "colab_type": "code",
        "outputId": "78157602-3869-4cc1-d7af-b7731f3d83df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.13.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: opensource@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: keras-preprocessing, astor, tensorflow-estimator, tensorboard, absl-py, wheel, gast, numpy, protobuf, keras-applications, six, termcolor, grpcio\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ9H1WLVY36i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgfxfJMJZX65",
        "colab_type": "code",
        "outputId": "be74edae-c831-4d97-ef98-32191295ab8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "# Dependencies\n",
        "import os \n",
        "import warnings \n",
        "#from absl import flags \n",
        "import matplotlib \n",
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp\n",
        "import math\n",
        "import pandas as pd\n",
        "tfd = tfp.distributions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n",
        "#import python_utils\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-BNIUMfZX_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clear graph (if any) before running \n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnIGZKOpZYCy",
        "colab_type": "code",
        "outputId": "e861c42b-3949-4631-8839-658753763acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "####Delete all flags before declare#####\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "flags.DEFINE_float(\"learning_rate\", default = 0.0001, help = \"Initial learning rate.\")\n",
        "flags.DEFINE_integer(\"epochs\", default = 700, help = \"Number of epochs to train for\")\n",
        "flags.DEFINE_integer(\"batch_size\", default =128, help = \"Batch size.\")\n",
        "flags.DEFINE_integer(\"eval_freq\", default = 400, help =\" Frequency at which to validate the model.\")\n",
        "flags.DEFINE_float(\"kernel_posterior_scale_mean\", default = -0.9, help = \"Initial kernel posterior mean of the scale (log var) for q(w)\")\n",
        "flags.DEFINE_float(\"kernel_posterior_scale_constraint\", default = 0.2, help = \"Posterior kernel constraint for the scale (log var) for q(w)\")\n",
        "flags.DEFINE_float(\"kl_annealing\", default = 50, help = \"Epochs to anneal the KL term (anneals from 0 to 1)\")\n",
        "flags.DEFINE_integer(\"num_hidden_layers\", default = 4, help = \"Number of hidden layers\")\n",
        "flags.DEFINE_integer(\"num_monte_carlo\",\n",
        "\n",
        "                     default=50,\n",
        "\n",
        "                     help=\"Network draws to compute predictive probabilities.\")\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "#initialize flags \n",
        "#FLAGS = flags.FLAGS\n",
        "print(FLAGS.learning_rate)\n",
        "print(FLAGS.epochs)\n",
        "print(FLAGS.num_monte_carlo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0001\n",
            "700\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG067Td8ZiIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_pipeline(X_train,X_test,y_train, y_test, batch_size, valid_size):\n",
        "  #Build an iterator over training batches \n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  #Shuffle the dataset (note shuffle argument much larger than training size)\n",
        "  # and form batches of size batch_size\n",
        "  training_batches = training_dataset.shuffle(20000, reshuffle_each_iteration =True).repeat().batch(batch_size)\n",
        "  training_iterator = tf.data.make_one_shot_iterator(training_batches)\n",
        "  \n",
        "  #Building iterator over the heldout set with batch_size = heldout_size,\n",
        "  # i.e., return the entire heldout set as a constant.\n",
        "  heldout_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
        "  heldout_iterator = tf.data.make_one_shot_iterator(heldout_batches)\n",
        "  \n",
        "  #Combine these into a feasible iterator that can switch between training \n",
        "  # and validation inputs.\n",
        "  # Here should be minibatch increment be defined \n",
        "  handle = tf.placeholder(tf.string, shape = [])\n",
        "  feedable_iterator = tf.data.Iterator.from_string_handle(handle, training_batches.output_types, training_batches.output_shapes)\n",
        "  features_final, labels_final = feedable_iterator.get_next()\n",
        "  \n",
        "  return features_final, labels_final, handle, training_iterator, heldout_iterator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcSdICmvZiQg",
        "colab_type": "code",
        "outputId": "0de5a21a-1424-4d2d-c15a-17f3f0b35fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QsAopJfZiUx",
        "colab_type": "code",
        "outputId": "c61e03ff-09e0-47ad-ae31-dc3111c169ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!ls \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " acc_1.csv   Bayesian_log_trial_.ipynb\t elvin.csv\t   Untitled1.ipynb\n",
            " acc_2.csv   BNN_Work.ipynb\t\t kernel.ipynb\t   work2.csv\n",
            " acc_3.csv  'Colab Notebooks'\t\t preds_2.npy\t   work_main\n",
            " acc_4.csv   data1.csv\t\t\t preds_3.npy\n",
            " acc_5.csv   data23.csv\t\t\t Untitled0.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15Q4z84sZYGq",
        "colab_type": "code",
        "outputId": "7d88b70a-3c85-4b3f-c594-2c74fd2ce7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read in the dataset\n",
        "df = pd.read_csv('/content/gdrive/My Drive/work2.csv').astype(np.float32)\n",
        "change = df.query('Speed>0').sample(frac = .2).index\n",
        "df.loc[change, 'Speed'] = 0\n",
        "df.loc[change, 'Class'] = 0\n",
        "df.to_csv('work2.csv', header = True, index =False)\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048575, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzLUkBo2Z1iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a0gG4bGZ1mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =1)\n",
        "\n",
        "#reshape y-data to become column vector \n",
        "y_train = np.reshape(y_train, [-1,1])\n",
        "y_test = np.reshape(y_test, [-1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yemecz6WZ1qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the dataset \n",
        "scalar_x_train = StandardScaler().fit(X_train)\n",
        "scalar_x_test = StandardScaler().fit(X_test)\n",
        "X_train = scalar_x_train.transform(X_train)\n",
        "X_test = scalar_x_test.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlT4rn13Z9nZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(argv):\n",
        "  (features_final, labels_final, handle, training_iterator, heldout_iterator) = build_input_pipeline(X_train,X_test, y_train,y_test, FLAGS.batch_size, 500)\n",
        "  \n",
        "  #features, labels = build_input_pipeline(X,y, FLAGS.batch_size)\n",
        "  #Building Bayesian Logistic \n",
        "  with tf.name_scope(\"logistic_regression\", values=[features_final]):\n",
        "    layer = tfp.layers.DenseFlipout(\n",
        "        units=1,\n",
        "        activation= tf.nn.relu,\n",
        "        kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),\n",
        "        bias_posterior_fn=tfp.layers.default_mean_field_normal_fn())\n",
        "    logits = layer(features_final)\n",
        "   # logits = layer(features)\n",
        "    labels_distribution = tfd.Bernoulli(logits=logits)\n",
        " #Compute the -ELBO as the loss, averaged over the batch size \n",
        "    t = tf.Variable(0.0)\n",
        "    kl_regularizer =  t / (FLAGS.kl_annealing * len(X_train) / FLAGS.batch_size)\n",
        "  \n",
        "    #Compute the -ELBO as the loss. The kl term is annealed from 1 to 1 over \n",
        "    # the epochs specified by the kl_annealing flag.\n",
        "    #log_likelihood = labels_distribution.log_prob(labels_final)\n",
        "    #log_likelihood = labels_distribution.log_prob(labels)\n",
        "   #neg_log_likelihood = tf.reduce_mean(tf.squared_difference(logits,labels_final))\n",
        "    neg_log_likelihood = -tf.reduce_mean(input_tensor = labels_distribution.log_prob(labels_final))\n",
        "    #neg_log_likelihood = -tf.reduce_mean(input_tensor = log_likelihood)\n",
        "    \n",
        "    #kl = sum(layer.losses)/len(X_train) * tf.minimum(1.0, kl_regularizer)\n",
        "    kl = tf.cast(sum(layer.losses)/len(X_train), tf.float32) * tf.minimum(1.0, kl_regularizer)\n",
        "    elbo_loss = neg_log_likelihood + kl\n",
        "   \n",
        "  \"\"\"neg_log_likelihood = -tf.reduce_mean(labels_distribution.log_prob(features))\n",
        "  kl = sum(layer.losses)/len(X_train)\n",
        "  elbo_loss = neg_log_likelihood + kl \"\"\"\n",
        "  #Build metrics for evaluation. Predictions are formed from single forward \n",
        "  #pass of the probabilistic layers. They are cheap but noise predictions\n",
        "  #predictions = tf.argmax(input = logits, axis=1)\n",
        "  #accuracy, accuracy_update_op = tf.metrics.accuracy(labels= labels_final,  predictions = predictions)\n",
        "  predictions = tf.cast(logits>0, dtype = tf.float32)\n",
        "  \n",
        "  \n",
        "  with tf.name_scope(\"train\"):\n",
        "    train_accuracy, train_accuracy_update_op = tf.metrics.accuracy(labels= labels_final,  predictions = predictions)\n",
        "    optimizer =tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "    train_op = optimizer.minimize(elbo_loss)\n",
        "    update_step_op = tf.assign(t, t+1)\n",
        "    #init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "  \n",
        "  with tf.name_scope(\"valid\"):\n",
        "    valid_accuracy, validation_accuracy_update_op = tf.metrics.accuracy(labels = labels_final,predictions = predictions)\n",
        "    #TP, TN, FP, FN\n",
        "    TP = tf.count_nonzero(predictions * labels_final)\n",
        "    TN = tf.count_nonzero((predictions-1)*(labels_final-1))\n",
        "    FP = tf.count_nonzero(predictions*(labels_final-1))\n",
        "    FN = tf.count_nonzero((predictions-1)* labels_final)\n",
        "    #precision, recall, f1\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "    f1 = 2 * precision * precision * recall / (precision + recall)\n",
        "    tpr = TP/(TP+FN)\n",
        "    fpr = FP/(TP+FN)\n",
        "   \n",
        "    init_op = tf.group(tf.global_variables_initializer(), \n",
        "                       tf.local_variables_initializer())\n",
        "    stream_vars_valid = [v for v in tf.local_variables() if \"valid\" in v.name]\n",
        "    reset_valid_op = tf.variables_initializer(stream_vars_valid)\n",
        "    \n",
        "    \n",
        "    \n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init_op)\n",
        "    #Run the training looop\n",
        "    # Fit the model to data.\n",
        "    train_handle = sess.run(training_iterator.string_handle())\n",
        "    heldout_handle = sess.run(heldout_iterator.string_handle())\n",
        "    training_steps = int(round(FLAGS.epochs * (len(X_train) / FLAGS.batch_size)))\n",
        "    for step in range(training_steps):\n",
        "      _ = sess.run([train_op,train_accuracy_update_op, update_step_op],feed_dict={handle: train_handle})\n",
        "      \n",
        "      # Manually print the frequency \n",
        "      if step % 100 == 0:\n",
        "        loss_value, accuracy_value, kl_value = sess.run([elbo_loss, train_accuracy, kl], feed_dict= {handle:train_handle})\n",
        "        print(\"Step:{:>3d} loss : {:.3f} KL: {:.3f}\" .format(step , loss_value, accuracy_value, kl_value))\n",
        "        \n",
        "      if (step +1) % FLAGS.eval_freq ==0: \n",
        "        # Compute log prob of heldout set by averaging draws from the model:\n",
        "        # p(heldout | train) = int_model p(heldout|model) p(model|train) ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
        "        # where model_i is a draw from the posterior \n",
        "        #p(model|train)\n",
        "        probs = np.asarray([sess.run((labels_distribution.probs), \n",
        "                                     feed_dict ={handle: heldout_handle})\n",
        "      \n",
        "                            for _ in range(FLAGS.num_monte_carlo)])\n",
        "     \n",
        "        mean_probs = np.mean(probs, axis =0).astype(np.int32)\n",
        "        print(mean_probs.dtype)\n",
        "        \n",
        "        _, label_vals = sess.run((features_final, labels_final), feed_dict = {handle: heldout_handle})\n",
        "        label_vals = (label_vals).astype(np.int32)\n",
        "      \n",
        "       # heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]), label_vals]))\n",
        "        \n",
        "        \n",
        "        #print(\" ...Held_out nats: {:.3f}\".format(heldout_lp))\n",
        "        \n",
        "       # Calculate validation accuracy\n",
        "\n",
        "        for  step in range(20):\n",
        "\n",
        "          sess.run(validation_accuracy_update_op, feed_dict={handle: heldout_handle})\n",
        "\n",
        "        #valid_value = sess.run(valid_accuracy, feed_dict={handle: heldout_handle})\n",
        "        loss_val_value, valid_accuracy_value, precision_value, recall_value, fpr_value, tpr_value,f1_value = sess.run([elbo_loss, valid_accuracy, precision, recall, fpr, tpr,f1], feed_dict = {handle:heldout_handle})\n",
        "        print(\"Step: {:>3d} elboloss: {:.3f} Validation Accuracy: {:.3f} Precision: {:.3f} Recall: {:.3f} fpr:{:.3f} tpr:{:.3f} f1:{:.3f} \".format(\n",
        "            step, loss_value, accuracy_value, precision_value, recall_value, fpr_value, tpr_value, f1_value))\n",
        "       # print(\"Step: {:>3d} fpr: {:.3f} tpr: {:.3f} \".format(\n",
        "           # step, fpr_value, tpr_value))\n",
        "        #loss_value, accuracy_value, precision_value, recall_value, fpr_value, tpr_value = sess.run([elbo_loss, accuracy, precision, recall, fpr, tpr], feed_dict = {handle: train_handle})\n",
        "        \n",
        "        #print( Validation Accuracy: {:.3f}\".format(valid_value))\n",
        "        \n",
        "        sess.run(reset_valid_op)\n",
        "if __name__ == \"__main__\": \n",
        "  tf.app.run()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNpQq-lqefJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHVwTYu-y_9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values\n",
        "y = y.reshape(-1, 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =1)\n",
        "\n",
        "#reshape y-data to become column vector \n",
        "y_train = np.reshape(y_train, [-1,1])\n",
        "y_test = np.reshape(y_test, [-1,1])\n",
        "\n",
        "# Standardize the dataset \n",
        "scalar_x_train = StandardScaler().fit(X_train)\n",
        "scalar_x_test = StandardScaler().fit(X_test)\n",
        "X_train = scalar_x_train.transform(X_train)\n",
        "X_test = scalar_x_test.transform(X_test)\n",
        "\n",
        "\n",
        "# create the training datasets\n",
        "dx_train = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "# apply a one-hot transformation to each label for use in the neural network\n",
        "dy_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "# zip the x and y training data together and shuffle, batch etc.\n",
        "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)\n",
        "data_iter = train_dataset.make_one_shot_iterator()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chq56kjy4A50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do the same operations for the validation set\n",
        "dx_valid = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "dy_valid = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)\n",
        "valid_iter= valid_dataset.make_one_shot_iterator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwC4GNm8z_HZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "c86c691b-98e3-4435-f1a4-2dff9ec3e964"
      },
      "source": [
        "#from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,  roc_curve\n",
        "#Build the network \n",
        "model = tf.keras.Sequential([\n",
        "   tf.keras.layers.Dense(16, input_dim=X_train.shape[1]),\n",
        "   tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "   tf.keras.layers.Dense(14, activation=tf.nn.relu),\n",
        "   tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
        "   tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "model.summary()\n",
        "# eager off\n",
        "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "model.compile( loss='binary_crossentropy',\n",
        "              optimizer= opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_dataset, steps_per_epoch=32, epochs=100,verbose=0 )\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(valid_dataset, steps=32)\n",
        "predictions = model.predict(valid_dataset)\n",
        "#y_train = tf.cast(y_train, tf.float32)\n",
        "predictions = tf.cast(predictions, tf.float32)\n",
        "\n",
        "for step, axis in enumerate([None, 0]):\n",
        "#TP, TN, FP, FN\n",
        "  TP = tf.count_nonzero(predictions * y_test)\n",
        "  TN = tf.count_nonzero((predictions-1)*(y_test-1))\n",
        "  FP = tf.count_nonzero(predictions*(y_test-1))\n",
        "  FN = tf.count_nonzero((predictions-1)* y_test)\n",
        "  #precision, recall, f1\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1 = 2 * precision * precision * recall / (precision + recall)\n",
        "  tpr = TP/(TP+FN)\n",
        "  fpr = FP/(TP+FN)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"loss:%f\"% (loss))\n",
        "#print(\"accuracy: %f\"%   (accuracy))\n",
        " \n",
        "print(\"val_accuracy: %f\"%   (accuracy))\n",
        "\n",
        "print(\"Step: {:>3d} elboloss: {:.3f} Validation Accuracy: {:.3f} Precision: {:.3f} Recall: {:.3f} fpr:{:.3f} tpr:{:.3f} f1:{:.3f}\".format(\n",
        "            step, loss_value, accuracy_value, precision_value, recall_value,fpr_value, tpr_value, f1_value))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_140 (Dense)            (None, 16)                320       \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 14)                238       \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 10)                150       \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 991\n",
            "Trainable params: 991\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 0.0372 - acc: 0.9885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-4a9c6c9d361a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#y_train = tf.cast(y_train, tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1096\u001b[0;31m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m       \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_steps_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m     \u001b[0mis_x_eager_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_steps_argument\u001b[0;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[1;32m    958\u001b[0m       raise ValueError('When using {input_type} as input to a model, you should'\n\u001b[1;32m    959\u001b[0m                        ' specify the `{steps_name}` argument.'.format(\n\u001b[0;32m--> 960\u001b[0;31m                            input_type=input_type_str, steps_name=steps_name))\n\u001b[0m\u001b[1;32m    961\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: When using iterators as input to a model, you should specify the `steps` argument."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1rvcP4wKdCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18026c27-bfd9-4e59-b5f9-142d51dc0887"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(838860, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emePvsQqImoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "clf = svm.SVC(kernel='rbf',degree=3, probability=False,tol=0.001)\n",
        "#clf = RandomForestClassifier(n_estimators=100,random_state=0)\n",
        "#clf = LogisticRegression(C=0.1)\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))  \n",
        "print(classification_report(y_test,y_pred))  \n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}